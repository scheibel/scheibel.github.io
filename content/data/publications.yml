heading: Publications
short: Publications
publications:
- key: 2014-thesis-evocells
  bibtex: s2014-thesis.bib
  title: >-
    Interaktive Visualisierung von hierarchischen, multivariaten und zeitlich
    variierenden Daten
  date: '2014-07-24'
  authors:
    - Willy Scheibel
  published: 'Master''s Thesis at the Hasso Plattner Institute, University of Potsdam'
  articletype: Thesis
  caption: >-
    A treemap recursively subdivides a two-dimensional area in order to encode
    a hierarchy and enables the visualization of multiple attributes e.g. with
    the size, the extruded height or the color of a node. Traditional treemap
    layout algorithms and rendering techniques can only be used for the
    comparison of two data sets at different points in time to some extent, as
    (1) no comparison between nodes in a treemap and between different states
    is possible and (2) there are no rendering techniques for the size
    differences of a node over time. This thesis introduces the techniques
    EvoCell-Layouting, Change Map, and Change Hints. EvoCell-Layouting is a
    novel treemap layout algorithm that iteratively changes a given treemap
    layout. Change Maps are density maps to locate changes in attribute values
    disregarding the difference and the size of the node. Change Hints
    visualize spatial changes between two states of a treemap. These three
    techniques enhance the comprehension of the evolution of temporal
    hierarchical data. A prototypical implementation, a discussion about
    alternatives, and performance and memory analyses demonstrate real data
    applicability. An additional case study reveals distinctive changes in the
    software system of a monitored open-source project that are hard to detect
    with traditional hierarchy visualizations.
  thumbnail: publications/2014-thesis-evocells.png
  downloads:
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/331399809/inline/jsViewer/5c77be7692851c695046e78a?pdfJsDownload=1
      desc: Document
  doi: 10.13140/RG.2.2.33837.33763
- key: 2016-small-multiples
  bibtex: std2016-small-multiples.bib
  title: Interactive Revision Exploration using Small Multiples of Software Maps
  date: '2016-02-28'
  authors:
    - Willy Scheibel
    - Matthias Trapp
    - Jürgen Döllner
  published:
    - venue: 7th International Conference on Information Visualization Theory and Applications
      short: IVAPP '16
      publisher: SciTePress
  articletype: Short Paper
  caption:
    To explore and to compare different revisions of complex software systems
    is a challenging task as it requires to constantly switch between
    different revisions and the corresponding information visualization. This
    paper proposes to combine the concept of small multiples and focus+context
    techniques for software maps to facilitate the comparison of multiple
    software map themes and revisions simultaneously on a single screen. This
    approach reduces the amount of switches and helps to preserve the mental
    map of the user. Given a software project the small multiples are based on
    a common dataset but are specialized by specific revisions and themes. The
    small multiples are arranged in a matrix where rows and columns represents
    different themes and revisions, respectively. To ensure scalability of the
    visualization technique we also discuss two rendering pipelines to ensure
    interactive frame-rates. The capabilities of the proposed visualization
    technique are demonstrated in a collaborative exploration setting using a
    high-resolution, multi-touch display.
  thumbnail: publications/2016-ivapp-small-multiples.png
  downloads:
    - href: 'https://doi.org/10.5220/0005694401310138'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Matthias_Trapp/publication/285164085/inline/jsViewer/56fbb61e08ae1b40b8062a54?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/331399816/inline/jsViewer/5c77ba33458515831f75e23b?pdfJsDownload=1
      desc: Poster (Portrait)
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/331399594/inline/jsViewer/5c77b9d0458515831f75e227?pdfJsDownload=1
      desc: Poster (Landscape)
  doi: 10.5220/0005694401310138
- key: 2016-decl3d-treemaps
  bibtex: lsld2016-decl3d-treemaps.bib
  title: Dynamic 2.5D Treemaps using Declarative 3D on the Web
  date: '2016-07-23'
  authors:
    - Daniel Limberger
    - Willy Scheibel
    - Stefan Lemme
    - Jürgen Döllner
  published:
    - venue: 21st International Conference on Web3D Technology
      short: Web3D '16
      publisher: ACM
  articletype: Short Paper
  caption: >-
    The 2.5D treemap represents a general purpose visualization technique to
    map multi-variate hierarchical data in a scalable, interactive, and
    consistent way used in a number of application fields. In this paper, we
    explore the capabilities of Declarative 3D for the web-based
    implementation of 2.5D treemap clients. Particularly, we investigate how
    X3DOM and XML3D can be used to implement clients with equivalent features
    that interactively display 2.5D treemaps with dynamic mapping of
    attributes. We also show a first step towards a glTF-based implementation.
    These approaches are benchmarked focusing on their interaction
    capabilities with respect to rendering and speed of dynamic data mapping.
    We discuss the results for our representative example of a complex 3D
    interactive visualization technique and summerize recommendations for
    improvements towards operational web clients.
  thumbnail: publications/2016-web3d-decl3d-treemaps.png
  downloads:
    - href: 'https://doi.org/10.1145/2945292.2945313'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/305420771/inline/jsViewer/5b2cb47b4585150d23c1f3ea?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Daniel_Limberger/publication/331407662/inline/jsViewer/5c77e4fa299bf1268d2c7e74?pdfJsDownload=1
      desc: Slides
    - href: 'https://github.com/cgcostume/web3d-treemaps'
      desc: Github Project
    - href: 'http://cgcostume.github.io/web3d-treemaps/'
      desc: Demo
  doi: 10.1145/2945292.2945313
- key: 2017-importance-based-aggregation
  bibtex: lshd2017-importance-based-aggregation.bib
  title: >-
    Reducing Visual Complexity in Software Maps using Importance-based
    Aggregation of Nodes
  date: '2017-02-27'
  authors:
    - Daniel Limberger
    - Willy Scheibel
    - Sebastian Hahn
    - Jürgen Döllner
  published:
    - venue: 8th International Conference on Information Visualization Theory and Applications
      short: IVAPP '17
      publisher: SciTePress
  articletype: Full Paper
  caption: >-
    Depicting massive software system data using software maps can result in
    visual clutter and increased cognitive load. This paper introduces an
    adaptive level-of-detail (LoD) technique that uses scoring for interactive
    aggregation on a per-node basis. The scoring approximates importance by
    degree-of-interest measures as well as screen and user-interaction scores.
    The technique adheres to established aggregation guidelines and was
    evaluated by means of two user studies. The first user study investigates
    task completion time in visual search. The second evaluates the
    readability of the presented nesting level contouring for aggregates. With
    the adap- tive LoD technique software maps allow for multi-resolution
    depictions of software system information. It facilitates efficient
    identification of important nodes and allows for additional annotation.
  thumbnail: publications/2017-importance-based-aggregation.png
  downloads:
    - href: 'https://doi.org/10.5220/0006267501760185'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Daniel_Limberger/publication/313377340/inline/jsViewer/589dbd3845851598bab4014b?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Daniel_Limberger/publication/331407406/inline/jsViewer/5c77deb8299bf1268d2c7c7b?pdfJsDownload=1
      desc: Slides
  doi: 10.5220/0006267501760185
- key: 2017-attributed-vertex-clouds
  bibtex: sbtd2017-attributed-vertex-clouds.bib
  title: Attributed Vertex Clouds
  date: '2017-05-15'
  authors:
    - Willy Scheibel
    - Stefan Buschmann
    - Matthias Trapp
    - Jürgen Döllner
  published: GPU Zen
  articletype: Chapter
  highlight: true
  caption: >-
    In todays computer graphics applications, large 3D scenes are rendered
    which consist of polygonal geometries such as triangle meshes. Using
    state-of-the-art techniques, this geometry is often represented on the GPU
    using vertex and index buffers, as well as additional auxiliary data such
    as textures or uniform buffers. For polygonal meshes of arbitrary
    complexity, the described approach is indispensable. However, there are
    several types of simpler geometries (e.g., cuboids, spheres, tubes, or
    splats) that can be generated procedurally. We present an efficient data
    representation and rendering concept for such geometries, denoted as
    attributed vertex clouds (AVCs). Using this approach, geometry is
    generated on the GPU during execution of the programmable rendering
    pipeline. Each vertex is used as the argument for a function that
    procedurally generates the target geometry. This function is called a
    transfer function, and it is implemented using shader programs and
    therefore executed as part of the rendering process. This approach allows
    for compact geometry representation and results in reduced memory
    footprints in comparison to traditional representations. By shifting
    geometry generation to the GPU, the resulting volatile geometry can be
    controlled flexibly, i.e., its position, parameterization, and even the
    type of geometry can be modified without requiring state changes or
    uploading new data to the GPU. Performance measurements suggests improved
    rendering times and reduced memory transmission through the rendering
    pipeline.
  thumbnail: publications/2017-gpuzen-attributed-vertex-clouds.png
  downloads:
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/313554437/inline/jsViewer/5c4afbe2299bf12be3e1f44a?pdfJsDownload=1
      desc: Author Version
    - href: 'https://github.com/hpicgs/attributedvertexclouds'
      desc: Github Project
- key: 2017-mixed-projection-treemaps
  bibtex: lstd2017-mixed-projection-treemaps.bib
  title: 'Mixed-Projection Treemaps: A Novel Approach Mixing 2D and 2.5D Treemaps'
  date: '2017-07-11'
  authors:
    - Daniel Limberger
    - Willy Scheibel
    - Matthias Trapp
    - Jürgen Döllner
  published:
    - venue: 21st International Conference on Information Visualisation
      short: IV '17
      publisher: IEEE
  articletype: Full Paper
  caption: >-
    2D treemaps are a space-filling visualization technique that facilitate
    exploration of non-spatial, attributed, tree-structured data using the
    visual variables size and color. In extension thereto, 2.5D treemaps
    introduce height for additional information display. This extension
    entails challenges such as increased rendering effort, occlusion, or the
    need for navigation techniques that counterbalance the advantages of 2D
    treemaps to a certain degree. This paper presents a novel technique for
    combining 2D and 2.5D treemaps using multi-perspective views to leverage
    the advantages of both treemap types. It enables a new form of
    overview+detail visualization for complex treemaps and contributes new
    concepts for real-time rendering of and interaction with mixed-projection
    treemaps. The technique operates by tilting up inner nodes using affine
    transformations and animated state transitions. The mixed use of
    orthogonal and perspective projections is discussed and application
    examples that facilitate exploration of multi-variate data and benefit
    from the reduced interaction overhead are demonstrated.
  thumbnail: publications/2017-iv-mixed-projection-treemaps.png
  downloads:
    - href: 'https://doi.org/10.1109/iV.2017.67'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Daniel_Limberger/publication/316999964/inline/jsViewer/5ab28be2aca272171001b92a?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Daniel_Limberger/publication/330106367/inline/jsViewer/5c77be80458515831f75e32b?pdfJsDownload=1
      desc: Slides
  doi: 10.1109/iV.2017.67
- key: 2018-evocells
  bibtex: swd2018-evocells.bib
  title: EvoCells – A Treemap Layout Algorithm for Evolving Tree Data
  date: '2018-01-29'
  authors:
    - Willy Scheibel
    - Christopher Weyand
    - Jürgen Döllner
  published:
    - venue: 9th International Conference on Information Visualization Theory and Applications
      short: IVAPP '18
      publisher: SciTePress
  articletype: Short Paper
  highlight: true
  caption: >-
    We propose the rectangular treemap layout algorithm EvoCells that maps
    changes in tree-structured data onto an initial treemap layout. Changes in
    topology and node weights are mapped to insertion, removal, growth, and
    shrinkage of the layout rectangles. Thereby, rectangles displace their
    neighbors and stretche their enclosing rectangles with a run-time
    complexity of O(n log n). An evaluation using layout stability metrics on
    the open source ElasticSearch software system suggests EvoCells as a valid
    alternative for stable treemap layouting.
  thumbnail: publications/2018-ivapp-evocells.png
  downloads:
    - href: 'https://doi.org/10.5220/0006617102730280'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/321059325/inline/jsViewer/5b2cb4964585150d23c1f41f?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/331399914/inline/jsViewer/5c77bc00299bf1268d2b8178?pdfJsDownload=1
      desc: Slides
  doi: 10.5220/0006617102730280
- key: 2019-hiviser
  bibtex: shd2019-hiviser.bib
  title: Design and Implementation of Web-Based Hierarchy Visualization Services
  date: '2019-02-28'
  authors:
    - Willy Scheibel
    - Judith Hartmann
    - Jürgen Döllner
  published:
    - venue: 10th International Conference on Information Visualization Theory and Applications
      short: IVAPP '19
      publisher: SciTePress
  note: Candidate for Best Paper
  articletype: Full Paper
  caption: >-
    There is a rapidly growing, cross-domain demand for interactive,
    high-quality visualization techniques as components of web-based
    applications and systems. In this context, a key question is how
    visualization services can be designed, implemented, and operated based on
    Software-as-a-Service as software delivery model. In this paper, we
    present concepts and design of a SaaS framework and API of visualization
    techniques for tree-structured data, called HiViSer. Using
    representational state transfer (REST), the API supports different data
    formats, data manipulations, visualization techniques, and output formats.
    In particular, the API defines base resource types for all components
    required to create an image or a virtual scene of a hierarchy
    visualization. We provide a treemap visualization service as prototypical
    implementation for which subtypes of the proposed API resources have been
    created. The approach generally serves as a blue-print for fully
    web-based, high-end visualization services running on thin clients in a
    standard browser environment.
  thumbnail: publications/2019-ivapp-hiviser.jpg
  downloads:
    - href: 'https://doi.org/10.5220/0007693201410152'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/330541206/inline/jsViewer/5c77b545a6fdcc4715a1c47d?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/331399770/inline/jsViewer/5c77b8c692851c695046e654?pdfJsDownload=1
      desc: Slides
    - desc: Homepage
      href: 'https://hiviser.varg.dev'
  doi: 10.5220/0007693201410152
- key: 2019-software-map-metaphors-and-techniques
  bibtex: lstd2019-software-map-metaphors-and-techniques.bib
  title: Advanced Visual Metaphors and Techniques for Software Maps
  date: '2019-10-01'
  authors:
    - Daniel Limberger
    - Willy Scheibel
    - Matthias Trapp
    - Jürgen Döllner
  published:
    - venue: 12th International Symposium on Visual Information Communication and Interaction
      short: VINCI '19
      publisher: ACM
  articletype: Full Paper
  caption: >-
    Software maps provide a general-purpose interactive user interface and
    information display for software analytics tools. This paper
    systematically introduces and classifies software maps as a treemap-based
    technique for software cartography. It provides an overview of advanced
    visual metaphors and techniques, each suitable for interactive visual
    analytics tasks, that can be used to enhance the expressiveness of
    software maps. Thereto, the metaphors and techniques are briefly
    described, located within a visualization pipeline model, and considered
    within the software map design space. Consequent applications and use
    cases w.r.t. different types of software system data and software
    engineering data are discussed, arguing for a versatile use of software
    maps in visual software analytics.
  thumbnail: publications/2019-vinci-software-map-metaphors-and-techniques.jpg
  downloads:
    - href: 'https://doi.org/10.1145/3356422.3356444'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Daniel_Limberger/publication/335062804/inline/jsViewer/5d883828a6fdcc8fd6106c95?pdfJsDownload=1
      desc: Author Version
  doi: 10.1145/3356422.3356444
- key: 2020-modular-hiviser
  bibtex: shld2020-modular-hiviser.bib
  title: Visualization of Tree-structured Data using Web Service Composition
  date: '2020-02-20'
  authors:
    - Willy Scheibel
    - Judith Hartmann
    - Daniel Limberger
    - Jürgen Döllner
  published:
    - venue: 'VISIGRAPP 2019: Computer Vision, Imaging and Computer Graphics Theory and Applications'
      short: VISIGRAPP 2019
      publisher: Springer
  articletype: Chapter
  caption: >-
    This article reiterates on the recently presented hierarchy visualization
    service HiViSer and its API. It illustrates its decomposition into modular
    services for data processing and visualization of tree-structured data.
    The decomposition is aligned to the common structure of visualization
    pipelines and, in this way, facilitates attribution of the services'
    capabilities. Suitable base resource types are proposed and their
    structure and relations as well as a subtyping concept for specifics in
    hierarchy visualization implementations are detailed. Moreover,
    state-of-the-art quality standards and techniques for self-documentation
    and discovery of components are incorporated. As a result, a blueprint for
    Web service design, architecture, modularization, and composition is
    presented, targeting fundamental visualization tasks of tree-structured
    data, i.e., gathering, processing, rendering, and provisioning. Finally,
    the applicability of the service components and the API is evaluated in
    the context of exemplary applications.
  thumbnail: publications/2019-ivapp-hiviser-extended.jpg
  downloads:
    - href: 'https://doi.org/10.1007/978-3-030-41590-7_10'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/339362147/inline/jsViewer/5e5ccf0c4585152ce8ff8d73?pdfJsDownload=1
      desc: Author Version
  doi: 10.1007/978-3-030-41590-7_10
- key: 2020-treemap-taxonomy
  bibtex: stld2020-treemap-taxonomy.bib
  title: A Taxonomy of Treemap Visualization Techniques
  date: '2020-02-27'
  authors:
    - Willy Scheibel
    - Matthias Trapp
    - Daniel Limberger
    - Jürgen Döllner
  published:
    - venue: 11th International Conference on Information Visualization Theory and Applications
      short: IVAPP '20
      publisher: SciTePress
  articletype: Position Paper
  highlight: true
  caption: >-
    A treemap is a visualization that has been specifically designed to
    facilitate the exploration of tree-structured data and, more general,
    hierarchically structured data. The family of visualization techniques
    that use a visual metaphor for parent-child relationships based “on the
    property of containment” (Johnson, 1993) is commonly referred to as
    treemaps. However, as the number of variations of treemaps grows, it
    becomes increasingly important to distinguish clearly between techniques
    and their specific characteristics. This paper proposes to discern between
    Space-filling Treemap, Containment Treemap, Implicit Edge Representation
    Tree, and Mapped Tree for classification of hierarchy visualization
    techniques and highlights their respective properties. This taxonomy is
    created as a hyponymy, i.e., its classes have an is-a relationship to one
    another. With this proposal, we intend to stimulate a discussion on a more
    unambiguous classification of treemaps and, furthermore, broaden what is
    understood by the concept of treemap itself.
  thumbnail: publications/2020-ivapp-treemap-taxonomy.png
  downloads:
    - href: 'https://doi.org/10.5220/0009153902730280'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/338402878/inline/jsViewer/5e5cd038a6fdccbeba12bf83?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/339617725/inline/jsViewer/5e5cd13ba6fdccbeba12bfea?pdfJsDownload=1
      desc: Slides
  doi: 10.5220/0009153902730280
- key: 2020-scatterplot
  bibtex: wsltd2020-scatterplot.bib
  title: >-
    A Framework for Interactive Exploration of Clusters in Massive Data using 3D
    Scatter Plots and WebGL
  date: '2020-12-01'
  authors:
    - Lukas Wagner
    - Willy Scheibel
    - Daniel Limberger
    - Matthias Trapp
    - Jürgen Döllner
  published:
    - venue: 25th International Conference on 3D Web Technology
      short: Web3D '20
      publisher: ACM
  articletype: Extended Abstract
  caption: >-
    This paper presents a rendering framework for the visualization of massive
    point datasets in the web. It includes highly interactive point rendering,
    cluster visualization, basic interaction methods, and importance-based
    labeling, while being available for both mobile and desktop browsers. The
    rendering style is customizable, as shown in figure 1. Our evaluation
    indicates that the framework facilitates interactive visualization of tens
    of millions of raw data points even without dynamic filtering or
    aggregation.
  thumbnail: publications/2020-web3d-scatterplot-renderer.png
  downloads:
    - href: 'https://doi.org/10.1145/3424616.3424730'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Lukas_Wagner18/publication/345770726/inline/jsViewer/5fbe75cb299bf104cf770132?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Lukas_Wagner18/publication/345770755/inline/jsViewer/5fad60e5299bf18c5b6aacb0?pdfJsDownload=1
      desc: Slides
    - href: 'https://varg.dev/web3d2020/'
      desc: Demo
  doi: 10.1145/3424616.3424730
- key: 2020-treemap-evaluation
  bibtex: fsltd2020-treemap-evaluation.bib
  title: Survey on User Studies on the Effectiveness of Treemaps
  date: '2020-12-02'
  authors:
    - Carolin Fiedler
    - Willy Scheibel
    - Daniel Limberger
    - Matthias Trapp
    - Jürgen Döllner
  published:
    - venue: 13th International Symposium on Visual Information Communication and Interaction
      short: VINCI '20
      publisher: ACM
  articletype: Full Paper
  highlight: true
  caption: >-
    Treemaps are a commonly used tool for the visual display and communication
    of tree-structured, multi-variate data. In order to confidently know when
    and how treemaps can best be applied, the research community uses
    usability studies and controlled experiments to "understand the potential
    and limitations of our tools" (Plaisant, 2004). To support the
    communities' understanding and usage of treemaps, this survey provides a
    comprehensive review and detailed overview of 69 user studies related to
    treemaps. However, due to pitfalls and shortcomings in design, conduct,
    and reporting of the user studies, there is little that can be reliably
    derived or accepted as a generalized statement. Fundamental open questions
    include configuration, compatible tasks, use cases, and perceptional
    characteristics of treemaps. The reliability of findings and statements is
    discussed and common pitfalls of treemap user studies are identified.
  thumbnail: publications/2020-vinci-treemap-evaluation.png
  downloads:
    - href: 'https://doi.org/10.1145/3430036.3430054'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/346565359/inline/jsViewer/60083702a6fdccdcb8690e02?pdfJsDownload=1
      desc: Author Version
    - href: 'https://varg-dev.github.io/treemap-studies'
      desc: Companion Website
  doi: 10.1145/3430036.3430054
- key: 2020-treemap-layouts
  bibtex: sld2020-treemap-layouts.bib
  title: Survey of Treemap Layout Algorithms
  date: '2020-12-03'
  authors:
    - Willy Scheibel
    - Daniel Limberger
    - Jürgen Döllner
  published:
    - venue: 13th International Symposium on Visual Information Communication and Interaction
      short: VINCI '20
      publisher: ACM
  articletype: Full Paper
  highlight: true
  caption: >-
    This paper provides an overview of published treemap layout algorithms
    from 1991 to 2019 that were used for information visualization and
    computational geometry. First, a terminology is outlined for the precise
    communication of tree-structured data and layouting processes. Second, an
    overview and classification of layout algorithms is presented and
    application areas are discussed. Third, the use-case-specific adaption
    process is outlined and discussed. This overview targets practitioners and
    researchers by providing a starting point for own research, visualization
    design, and applications.
  thumbnail: publications/2020-vinci-treemap-layouts.png
  downloads:
    - href: 'https://doi.org/10.1145/3430036.3430041'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/346565429/inline/jsViewer/6008e5e9a6fdccdcb86bb8d0?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/348647547/inline/jsViewer/6008e45f299bf14088adae8b?pdfJsDownload=1
      desc: Slides
  doi: 10.1145/3430036.3430041
- key: 2021-software-forest
  bibtex: achssld2021-software-forest.bib
  title: >-
    Software Forest: A Visualization of Semantic Similarities in Source Code
    using a Tree Metaphor
  date: '2021-02-08'
  authors:
    - Daniel Atzberger
    - Tim Cech
    - Merlin de la Haye
    - Maximilian Söchting
    - Willy Scheibel
    - Daniel Limberger
    - Jürgen Döllner
  published:
    - venue: 12th International Conference on Information Visualization Theory and Applications
      short: IVAPP '21
      publisher: SciTePress
  note: Candidate for Best Student Paper
  articletype: Full Paper
  caption: >-
    Software visualization techniques provide effective means for program
    comprehension tasks as they allow developers to interactively explore
    large code bases. A frequently encountered task during software
    development is the detection of source code files of similar semantic. To
    assist this task we present Software Forest, a novel 2.5D software
    visualization that enables interactive exploration of semantic
    similarities within a software system, illustrated as a forest. The
    underlying layout results from the analysis of the vocabulary of the
    software documents using Latent Dirichlet Allocation and Multidimensional
    Scaling and therefore reflects the semantic similarity between source code
    files. By mapping properties of a software entity, e.g., size metrics or
    trend data, to visual variables encoded by various, figurative tree
    meshes, aspects of a software system can be displayed. This concept is
    complemented with implementation details as well as a discussion on
    applications.
  thumbnail: publications/2021-ivapp-software-forest.png
  downloads:
    - href: 'https://doi.org/10.5220/0010267601120122'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/349382985/inline/jsViewer/6047926192851c077f2981ce?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/365824554/inline/jsViewer/63f5c80a0d98a97717ab5b9a?pdfJsDownload=1
      desc: Slides
    - href: 'https://varg.dev/trees/'
      desc: Demo
  doi: 10.5220/0010267601120122
- key: 2021-hilbert-moore-performance
  bibtex: swbd2021-hilbertmooreperformance.bib
  title: >-
    Algorithmic Improvements on Hilbert and Moore Treemaps for Visualization of
    Large Tree-structured Datasets
  date: '2021-06-14'
  authors:
    - Willy Scheibel
    - Christopher Weyand
    - Joseph Bethge
    - Jürgen Döllner
  published:
    - venue: 23rd EG Conference on Visualization
      short: EuroVis '21
      publisher: EG
  articletype: Short Paper
  highlight: true
  caption: >-
    Hilbert and Moore treemaps are based on the same named space-filling
    curves to lay out tree-structured data for visualization. One main
    component of them is a partitioning subroutine, whose algorithmic
    complexity poses problems when scaling to industry-sized datasets.
    Further, the subroutine allows for different optimization criteria that
    result in different layout decisions. This paper proposes conceptual and
    algorithmic improvements to this partitioning subroutine. Two measures for
    the quality of partitioning are proposed, resulting in the min-max and
    min-variance optimization tasks. For both tasks, linear-time algorithms
    are presented that find an optimal solution. The implementation variants
    are evaluated with respect to layout metrics and run-time performance
    against a previously available greedy approach. The results show
    significantly improved run time and no deterioration in layout metrics,
    suggesting effective use of Hilbert and Moore treemaps for datasets with
    millions of nodes.
  thumbnail: publications/2021-eurovis-hilbert-moore-treemaps.png
  downloads:
    - href: 'https://doi.org/10.2312/evs.20211065'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/351103336/inline/jsViewer/60a393a8a6fdccb8dc632568?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy_Scheibel/publication/351659477/inline/jsViewer/60a3952c458515952dd44930?pdfJsDownload=1
      desc: Slides
    - href: 'https://github.com/varg-dev/hilbert-moore-treemap-layouts-prototype'
      desc: Github Project
  doi: 10.2312/evs.20211065
- key: 2021-procedural-textures
  bibtex: lsdd2021-procedural-textures.bib
  title: >-
    Visualization of Data Changes in 2.5D Treemaps using Procedural Textures and
    Animated Transitions
  date: '2021-09-06'
  authors:
    - Daniel Limberger
    - Willy Scheibel
    - Jan van Dieken
    - Jürgen Döllner
  published:
    - venue: 14th International Symposium on Visual Information Communication and Interaction
      short: VINCI '21
      publisher: ACM
  articletype: Short Paper
  caption: >-
    This work investigates the extent to which animated procedural texture
    patterns can be used to support the representation of changes in 2.5D
    treemaps. Changes in height, color, and area of individual nodes can
    easily be visualized using animated transitions. Especially for changes in
    the color attribute, plain animated transitions are not able to directly
    communicate the direction of change itself. We show how procedural texture
    patterns can be superimposed to the color mapping and support transitions.
    To this end, we discuss qualitative properties of each pattern,
    demonstrate their ability to communicate change direction both with and
    without animation, and conclude which of the patterns are more likely to
    increase effectiveness and correctness of the change mapping in 2.5D
    treemaps.
  thumbnail: publications/2021-vinci-procedural-textures.png
  downloads:
    - href: 'https://doi.org/10.1145/3481549.3481570'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/354331984/inline/jsViewer/61486d013c6cb310697e65f0&pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/368690808/inline/jsViewer/63f5d26db1704f343f72e931?pdfJsDownload=1
      desc: Slides
  doi: 10.1145/3481549.3481570
- key: 2021-etf-visualization
  bibtex: blsd2021-etf-visualization.bib
  title: >-
    Interactive Simulation and Visualization of Long-Term, ETF-based Investment
    Strategies
  date: '2021-09-06'
  authors:
    - Martin Büßemeyer
    - Daniel Limberger
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 14th International Symposium on Visual Information Communication and Interaction
      short: VINCI '21
      publisher: ACM
  articletype: Short Paper
  caption: >-
    Personal, long-term investment products, especially ones for retirement
    savings, require thorough understanding to use them profitably. Even
    simple savings plans based on exchange-traded funds(ETFs) are subject to
    many variables and uncertainties to be considered for expected and
    planned-upon returns. We present aninteractive simulation of an ETF-based
    savings plan that combinesforecasts, risk awareness, taxes and costs,
    inflation, and dynamicinflows and outflows into a single visualization.
    The visualizationconsists of four parts: a form-fill interface for
    configuration, a savings and payout simulation, a cash flow chart, and a
    savings chart. Based on a specific use case, we discuss how private
    investors canbenefit from using our visualization after a short training
    period.
  thumbnail: publications/2021-vinci-etf-visualization.png
  downloads:
    - href: 'https://doi.org/10.1145/3481549.3481568'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/354332166/inline/jsViewer/61486d4e519a1a381f702692&pdfJsDownload=1
      desc: Author Version
    - href: 'https://etf-vis.net/?apiKey=ew5r345z43'
      desc: Demo
  doi: 10.1145/3481549.3481568
- key: 2021-softwaregalaxy
  bibtex: asld2021-softwaregalaxy.bib
  title: 'Software Galaxies: Displaying Coding Activities using a Galaxy Metaphor'
  date: '2021-09-07'
  authors:
    - Daniel Atzberger
    - Willy Scheibel
    - Daniel Limberger
    - Jürgen Döllner
  published:
    - venue: 14th International Symposium on Visual Information Communication and Interaction
      short: VINCI '21
      publisher: ACM
  articletype: Extended Abstract
  caption: >-
    Software visualization uses metaphors to depict software and software
    development data that usually has no gestalt. The choice of a metaphor and
    visual depiction is researched broadly, but deriving a layout based on
    similarity is still challenging. We present a novel approach to 3D
    software visualization called Software Galaxy. Our layout is based on
    applying Latent Dirichlet Allocation on source code. We utilize a metaphor
    inspired from astronomy for depicting software metrics for single files
    and clusters. Our first experiments indicate that a 3D visualization
    capturing semantic relatedness can be beneficial for standard program
    comprehension tasks.
  thumbnail: publications/2021-vinci-softwaregalaxy.png
  downloads:
    - href: 'https://doi.org/10.1145/3481549.3481573'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/354332172/inline/jsViewer/61486d33519a1a381f70268d&pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/365824584/inline/jsViewer/63f5c8dc0d98a97717ab6258?pdfJsDownload=1
      desc: Slides
  doi: 10.1145/3481549.3481573
- key: 2022-knowhowmap
  bibtex: acjsltd2022-knowhowmap.bib
  title: >-
    Visualization of Knowledge Distribution across Development Teams using 2.5D
    Semantic Software Maps
  date: '2022-02-06'
  authors:
    - Daniel Atzberger
    - Tim Cech
    - Adrian Jobst
    - Willy Scheibel
    - Daniel Limberger
    - Matthias Trapp
    - Jürgen Döllner
  published:
    - venue: 13th International Conference on Information Visualization Theory and Applications
      short: IVAPP '22
      publisher: SciTePress
  articletype: Short Paper
  caption: >-
    In order to detect software risks at an early stage, various software
    visualization techniques have been developed for monitoring the structure,
    behaviour, or the underlying development process of software. One of
    greatest risks for any IT organization consists in an inappropriate
    distribution of knowledge among its developers, as a projects' success
    mainly depends on assigning tasks to developers with the required skills
    and expertise. In this work, we address this problem by proposing a novel
    Visual Analytics framework for mining and visualizing the expertise of
    developers based on their source code activities. Under the assumption
    that a developer's knowledge about code is represented directly through
    comments and the choice of identifier names, we generate a 2D layout using
    Latent Dirichlet Allocation together with Multidimensional Scaling on the
    commit history, thus displaying the semantic relatedness between
    developers. In order to capture a developer's expertise in a concept, we
    utilize Labeled LDA trained on a corpus of Open Source projects. By
    mapping aspects related to skills onto the visual variables of 3D glyphs,
    we generate a 2.5D Visualization, we call KnowhowMap. We exemplify this
    approach with an interactive prototype that enables users to analyze the
    distribution of skills and expertise in an explorative way.
  thumbnail: publications/2022-ivapp-knowhowmap.png
  downloads:
    - href: 'https://doi.org/10.5220/0010991100003124'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Matthias-Trapp-2/publication/358523984/inline/jsViewer/62061c62afa8884cabd8675c?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Matthias-Trapp-2/publication/361585105/inline/jsViewer/62badc31056dae24e8e97d65?pdfJsDownload=1
      desc: Slides
  doi: 10.5220/0010991100003124
- key: 2022-authortopicmodel
  bibtex: assltd2022-authortopicmodel.bib
  title: >-
    Mining Developer Expertise from Bug Tracking Systems using the Author-Topic
    Model
  date: '2022-04-25'
  authors:
    - Daniel Atzberger
    - Jonathan Schneider
    - Willy Scheibel
    - Daniel Limberger
    - Matthias Trapp
    - Jürgen Döllner
  published:
    - venue: 17th International Conference on Evaluation of Novel Approaches to Software Engineering
      short: ENASE '22
      publisher: SciTePress
  note: Best Student Paper Award
  highlight: true
  articletype: Full Paper
  caption: >-
    During software development processes, software defects, so-called bugs,
    are captured in a semi-structured manner in bug tracking systems using
    textual components and categorical features. It is the task of the triage
    owner to assign open bugs to developers with the required skills and
    expertise. This task, known as bug triaging, requires an in-depth
    knowledge about a developer's skills. Various machine learning techniques
    have been proposed to automate this task, most of these approaches apply
    topic models, especially Latent Dirichlet Allocation (LDA), for mining the
    textual components of bug reports. However none of the proposed approaches
    explicitly models a developers expertise. In most cases these algorithms
    are treated as black box, as they allow no explanation about their
    recommendation. In this work, we show how the Author-Topic Model (ATM), a
    variant of LDA, can be used to capture a developer's expertise in the
    latent topics of a corpus of bug reports from the model itself.
    Furthermore, we present three novel bug triaging techniques based on the
    ATM. We compare our approach against a baesline model, that is based on
    LDA, on a dataset of 18269 bug reports from the Mozilla Firefox project
    collected between July 1999 to June 2016. The results show that the ATM
    can outperform the LDA-based approach in terms of the Mean Reciprocal Rank
    (MRR).
  thumbnail: publications/2022-enase-authortopicmodel.png
  downloads:
    - href: 'https://doi.org/10.5220/0011045100003176'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/360230384/inline/jsViewer/626ba74705d79a3968aa8968?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/365824562/inline/jsViewer/63f5c98f574950594534bad7?pdfJsDownload=1
      desc: Slides
  doi: 10.5220/0011045100003176
- key: 2022-downstreamdependencies
  bibtex: tsld2022-downstreamdependencies.bib
  title: >-
    Augmenting Library Development by Mining Usage Data from Downstream
    Dependencies
  date: '2022-04-25'
  authors:
    - Christoph Thiede
    - Willy Scheibel
    - Daniel Limberger
    - Jürgen Döllner
  published:
    - venue: 17th International Conference on Evaluation of Novel Approaches to Software Engineering
      short: ENASE '22
      publisher: SciTePress
  note: Candidate for Best Student Paper
  articletype: Full Paper
  caption: >-
    In the dependency graph of a software ecosystem, downstream dependencies
    are the nodes that depend on a package. Apart from end-user APIs, these
    dependencies make up the bulk of a library's usage for most packages.
    Other than for upstream dependencies, tools that provide individual
    package developers with this kind of information rarely exist to date.
    This paper makes two contributions: (i) We propose an approach for
    gathering downstream dependencies of a single package efficiently and
    extracting usage samples from them using a static type analyzer. (ii) We
    present a tool that allows npm package developers to survey the aggregated
    usage data directly in their IDE in an interactive and context-sensitive
    way and that further supports them in understanding which packages use
    specific package members and why and how they use these members. This can
    help prioritize and steer development and uncover unexpected usage
    patterns, inappropriate member signatures, or misleading interface design.
    Our methods return over 8000 dependencies for popular packages and process
    about 12 dependencies per minute while requiring about 500 MB memory in
    total and less than 30 MB storage per package, but tend to exclude
    unpopular dependencies. Usage sample extraction is very precise but not
    easily available for repositories with complex build configurations or
    metaprogramming patterns. We show that usage data from downstream
    dependency repositories is a promising and diverse source of information
    for mining software repositories and that our approach supports package
    developers in maintaining their APIs.
  thumbnail: publications/2022-enase-downstream-mining.png
  downloads:
    - href: 'https://doi.org/10.5220/0011093700003176'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/360231022/inline/jsViewer/626ba510bfd24037e9dd1eee?pdfJsDownload=1
      desc: Author Version
    - href: 'https://github.com/LinqLover/downstream-repository-mining'
      desc: Github Project
    - href: 'https://doi.org/10.5281/zenodo.6338060'
      desc: Source Code Archive
  doi: 10.5220/0011093700003176
- key: 2022-gitmining
  bibtex: hsd2022-gitmining.bib
  title: Tooling for Time- and Space-efficient git Repository Mining
  date: '2022-05-23'
  authors:
    - Fabian Heseding
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 19th International Conference on Mining Software Repositories – Data and Tool Showcase Track
      short: MSR '22
      publisher: ACM
  articletype: Short Paper
  highlight: true
  caption: >-
    Software projects under version control grow with each commit,
    accumulating up to hundreds of thousands of commits per repository.
    Especially for such large projects, the traversal of a repository and data
    extraction for static source code analysis poses a trade-off between
    granularity and speed. We showcase the command-line tool pyrepositoryminer
    that combines a set of optimization approaches for efficient traversal and
    data extraction from git repositories while being adaptable to third-party
    and custom software metrics and data extractions. The tool is written in
    Python and combines bare repository access, in-memory storage,
    parallelization, caching, change-based analysis, and optimized
    communication between the traversal and custom data extraction components.
    The tool allows for both metrics written in Python and external programs
    for data extraction. A single-thread performance evaluation based on a
    basic mining use case shows a mean speedup of 15.6x to other freely
    available tools across four mid-sized open source projects. A
    multi-threaded execution allows for load distribution among cores and,
    thus, a mean speedup up to 86.9x using 12 threads.
  thumbnail: publications/2022-msr-git-mining.png
  downloads:
    - href: 'https://doi.org/10.1145/3524842.3528503'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/360243146/inline/jsViewer/626bab128e7e1e5d5fa39457?pdfJsDownload=1
      desc: Author Version
    - href: 'https://github.com/fabianhe/pyrepositoryminer'
      desc: Github Project
    - href: 'https://doi.org/10.5281/zenodo.5918480'
      desc: Source Code Archive
    - href: 'https://doi.org/10.48550/arXiv.2205.01351'
      desc: Preprint
  doi: 10.1145/3524842.3528503
- key: 2022-prometheus
  bibtex: jacstd2022-prometheus.bib
  title: Efficient GitHub Crawling using the GraphQL API
  date: '2022-07-04'
  authors:
    - Adrian Jobst
    - Daniel Atzberger
    - Tim Cech
    - Willy Scheibel
    - Matthias Trapp
    - Jürgen Döllner
  published:
    - venue: 22th International Conference on Computational Science and Its Applications
      short: ICCSA '22
      publisher: Springer
  articletype: Full Paper
  caption: >-
    The number of publicly accessible software repositories on online
    platforms is growing rapidly. With more than 128 million public
    repositories (as of March 2020), GitHub is the world's largest platform
    for hosting and managing software projects. Where it used to be necessary
    to merge various data sources, it is now possible to access a wealth of
    data using the GitHub API alone. However, collecting and analyzing this
    data is not an easy endeavor. In this paper, we present Prometheus, a
    system for crawling and storing software repositories from GitHub.
    Compared to existing frameworks, Prometheus follows an event-driven
    microservice architecture. By separating functionality on the service
    level, there is no need to understand implementation details or use
    existing frameworks to extend or customize the system, only data.
    Prometheus consists of two components, one for fetching GitHub data and
    one for data storage which serves as a basis for future functionality.
    Unlike most existing crawling approaches, the Prometheus fetching service
    uses the GitHub GraphQL API. As a result, Prometheus can significantly
    outperform alternatives in terms of throughput in some scenarios.
  thumbnail: publications/2022-iccsa-prometheus.png
  downloads:
    - href: 'https://doi.org/10.1007/978-3-031-10548-7_48'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/361810170/inline/jsViewer/63f5c6200cf1030a5641799f?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/365824474/inline/jsViewer/63f5c66bb1704f343f72aad0?pdfJsDownload=1
      desc: Slides
  doi: 10.1007/978-3-031-10548-7_48
- key: 2022-topicmodel-benchmark
  bibtex: acsldt2022-topicmap-benchmark.bib
  title: A Benchmark for the Use of Topic Models for Text Visualization Tasks
  date: '2022-08-16'
  authors:
    - Daniel Atzberger
    - Tim Cech
    - Willy Scheibel
    - Daniel Limberger
    - Matthias Trapp
    - Jürgen Döllner
  published:
    - venue: 15th International Symposium on Visual Information Communication and Interaction
      short: VINCI '22
      publisher: ACM
  articletype: Extended Abstract
  caption: ''
  thumbnail: publications/2022-vinci-topicmodel-benchmark.png
  downloads:
    - href: 'https://doi.org/10.1145/3554944.3554961'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Matthias-Trapp-2/publication/363174697/inline/jsViewer/63452011ff870c55ce17f64d?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/365824462/inline/jsViewer/63f5c6ba0cf1030a56417ef5?pdfJsDownload=1
      desc: Slides
  doi: 10.1145/3554944.3554961
- key: 2022-configurationsoftwaremaps
  bibtex: lsdt2022-configurationsoftwaremaps.bib
  title: Visual Variables and Configuration of Software Maps
  date: '2022-09-16'
  authors:
    - Daniel Limberger
    - Willy Scheibel
    - Jürgen Döllner
    - Matthias Trapp
  published:
    - venue: Journal of Visualization
      short: JoVi
      publisher: Springer
  articletype: Journal Article
  highlight: true
  caption: >-
    Software maps provide a general-purpose interactive user interface and
    information display in software analytics. This paper classifies software
    maps as a containment-based treemap embedded into a 3D attribute space and
    introduces respective terminology. It provides a comprehensive overview of
    advanced visual metaphors and techniques, each suitable for interactive
    visual analytics tasks. The metaphors and techniques are briefly
    described, located within a visualization pipeline model, and considered
    within a software map design space. The general expressiveness and
    applicability of visual variables are detailed and discussed. Consequent
    applications and use cases w.r.t. different types of software system data
    and software engineering data are discussed, arguing for versatile use of
    software maps in visual software analytics.
  thumbnail: publications/2022-jov-configurationsoftwaremaps.png
  downloads:
    - href: 'https://doi.org/10.1007/s12650-022-00868-1'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/363174603/inline/jsViewer/63242c14873eca0c008f32bf?pdfJsDownload=1
      desc: Author Version
  doi: 10.1007/s12650-022-00868-1
- key: 2022-codecv
  bibtex: ascstd2022-codecv.bib
  title: 'CodeCV: Mining Expertise of GitHub Users from Coding Activities'
  date: '2022-10-03'
  authors:
    - Daniel Atzberger
    - Nico Scordialo
    - Tim Cech
    - Willy Scheibel
    - Matthias Trapp
    - Jürgen Döllner
  published:
    - venue: 22nd International Working Conference on Source Code Analysis and Manipulation
      short: SCAM '22
      publisher: IEEE
  articletype: Short Paper
  caption: >-
    The number of software projects developed collaboratively on social coding
    platforms is steadily increasing. One of the motivations for developers to
    participate in open-source software development is to make their
    development activities easier accessible to potential employers, e.g., in
    the form of a resume for their interests and skills. However, manual
    review of source code activities is time-consuming and requires detailed
    knowledge of the technologies used. Existing approaches are limited to a
    small subset of actual source code activity and metadata and do not
    provide explanations for their results. In this work, we present CodeCV,
    an approach to analyzing the commit activities of a GitHub user concerning
    the use of programming languages, software libraries, and higher-level
    concepts, e.g., Machine Learning or Cryptocurrency. Skills in using
    software libraries and programming languages are analyzed based on
    syntactic structures in the source code. Based on Labeled Latent Dirichlet
    Allocation, an automatically generated corpus of GitHub projects is used
    to learn the concept-specific vocabulary in identifier names and comments.
    This enables the capture of expertise on abstract concepts from a user's
    commit history. CodeCV further explains the results through links to the
    relevant commits in an interactive web dashboard. We tested our system on
    selected GitHub users who mainly contribute to popular projects to
    demonstrate that our approach is able to capture developers' expertise
    effectively.
  thumbnail: publications/2022-scam-codecv.png
  downloads:
    - href: 'https://doi.org/10.1109/SCAM55253.2022.00021'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/363174745/inline/jsViewer/63f5d41e0d98a97717aba29d?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/365824568/inline/jsViewer/63f5c7100d98a97717ab52bf?pdfJsDownload=1
      desc: Slides
  doi: 10.1109/SCAM55253.2022.00021
- key: 2022-procedural-texture-patterns
  bibtex: lsdd2022-procedural-texture-patterns.bib
  title: >-
    Procedural Texture Patterns for Encoding Changes in Color in 2.5D Treemap
    Visualizations
  date: '2022-10-03'
  authors:
    - Daniel Limberger
    - Willy Scheibel
    - Jan van Diecken
    - Jürgen Döllner
  published:
    - venue: Journal of Visualization
      short: JoVi
      publisher: Springer
  highlight: true
  articletype: Journal Article
  caption: >-
    Treemaps depict tree-structured data while maintaining flexibility in
    mapping data to different visual variables. This work explores how changes
    in data mapped to color can be represented with rectangular 2.5D treemaps
    using procedural texture patterns. The patterns are designed to function
    for both static images and interactive visualizations with animated
    transitions. During rendering, the procedural texture patterns are
    superimposed onto the existing color mapping. We present a pattern catalog
    with seven exemplary patterns having different characteristics in
    representing the mapped data. This pattern catalog is implemented in a
    WebGL-based treemap rendering prototype and is evaluated using performance
    measurements and case studies on two software projects. As a result, this
    work extends the toolset of visual encodings for 2.5D treemaps by
    procedural texture patterns to represent changes in color. It serves as a
    starting point for user-centered evaluation.
  thumbnail: publications/2022-jov-procedural-texture-patterns.png
  downloads:
    - href: 'https://doi.org/10.1007/s12650-022-00874-3'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/publication/363174833/inline/jsViewer/633c3e00ff870c55cefe2861?pdfJsDownload=1
      desc: Author Version
  doi: 10.1007/s12650-022-00874-3
- key: 2022-scatterplot-projections
  bibtex: wsld2022-scatterplot-projections.bib
  title: >-
    Hardware-accelerated Rendering of Web-based 3D Scatter Plots with Projected
    Density Fields and Embedded Controls
  date: '2022-11-02'
  authors:
    - Lukas Wagner
    - Daniel Limberger
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 27th International Conference on 3D Web Technology
      short: Web3D '22
      publisher: ACM
  note: Best Paper Award
  highlight: true
  articletype: Full Paper
  doi: 10.1145/3564533.3564566
  caption: >-
    3D scatter plots depicting massive data suffer from occlusion, which makes
    it difficult to get an overview and perceive structure.  This paper
    presents a technique that facilitates the comprehension of heavily
    occluded 3D scatter plots. Data points are projected to axial planes,
    creating x-ray-like 2D views that support the user in analyzing the data's
    density and layout. We showcase our open-source web application with a
    hardware-accelerated rendering component written in WebGL. It allows for
    interactive interaction, filtering, and navigation with datasets up to
    hundreds of thousands of nodes. The implementation is detailed and
    discussed with respect to challenges posed by API and performance
    limitations.
  thumbnail: publications/2022-web3d-scatterplot-projections.png
  downloads:
    - href: 'https://doi.org/10.1145/3564533.3564566'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/364806579/inline/jsViewer/636d0b912f4bca7fd04bb4a0?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Lukas-Wagner-17/publication/365319445/inline/jsViewer/636ed5e937878b3e87a30f62?pdfJsDownload=1
      desc: Slides
    - href: 'https://demo.varg.dev/vidi/web3d-2022'
      desc: Demo
    - href: 'https://github.com/lukaswagner/vidi'
      desc: Github Project
- key: 2023-semantic-software-maps
  bibtex: acsld2023-semantic-software-maps.bib
  title: Visualization of Source Code Similarity using 2.5D Semantic Software Maps
  date: '2023-02-02'
  authors:
    - Daniel Atzberger
    - Tim Cech
    - Willy Scheibel
    - Daniel Limberger
    - Jürgen Döllner
  published:
    - venue: 'VISIGRAPP 2021: Computer Vision, Imaging and Computer Graphics Theory and Applications'
      short: VISIGRAPP 2021
      publisher: Springer
  articletype: Chapter
  highlight: true
  caption: >-
    For various program comprehension tasks, software visualization techniques
    can be beneficial by displaying aspects related to the behavior,
    structure, or evolution of software. In many cases, the question is
    related to the semantics of the source code files, e.g., the localization
    of files that implement specific features or the detection of files with
    similar semantics. This work presents a general software visualization
    technique for source code documents, which uses 3D glyphs placed on a
    two-dimensional reference plane. The relative positions of the glyphs
    captures their semantic relatedness. Our layout originates from applying
    Latent Dirichlet Allocation and Multidimensional Scaling on the comments
    and identifier names found in the source code files. Though different
    variants for 3D glyphs can be applied, we focus on cylinders, trees, and
    avatars. We discuss various mappings of data associated with source code
    documents to the visual variables of 3D glyphs for selected use cases and
    provide details on our visualization system.
  thumbnail: publications/2023-ivapp-software-forest-extended.png
  downloads:
    - href: 'https://doi.org/10.1007/978-3-031-25477-2_8'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/363175014/inline/jsViewer/63f5c7c4574950594534ab35?pdfJsDownload=1
      desc: Author Version
  doi: 10.1007/978-3-031-25477-2_8
- key: 2023-probabilistic-models
  bibtex: assdt2023-probabilistic-models.bib
  title: Evaluating Probabilistic Topic Models for Bug Triaging Tasks
  date: '2023-03-01'
  authors:
    - Daniel Atzberger
    - Jonathan Schneider
    - Willy Scheibel
    - Matthias Trapp
    - Jürgen Döllner
  published:
    - venue: 'ENASE 2022: Evaluation of Novel Approaches to Software Engineering'
      short: ENASE 2022
      publisher: Springer
  articletype: Chapter
  caption: >-
    During the software development process, occurring problems are collected
    and managed as bug reports using bug tracking systems. Usually, a bug
    report is specified by a title, a more detailed description, and
    additional categorical information, e.g., the affected component or the
    reporter. It is the task of the triage owner to assign open bug reports to
    developers with the required skills to fix them. However, the bug
    assignment task is time-consuming, especially in large software projects
    with many involved developers. This observation motivates using
    (semi-)automatic algorithms for assigning bugs to developers. Various
    approaches have been developed that rely on a machine learning model
    trained on historical bug reports. Thereby, the modeling of the textual
    components is mainly done using topic models, mainly Latent Dirichlet
    Allocation (LDA). Although different variants, inference techniques, and
    libraries for LDA exist and various hyperparameters can be specified, most
    works treat topic models as a black box without exploring them in detail.
    In this work, we extend a study of Atzberger and Schneider et al. on the
    use of the Author-Topic Model (ATM) for bug triaging tasks. We demonstrate
    the influence of the underlying topic model, the used library and
    inference techniques, and the hyperparameters on the bug triaging results.
    The results of our conducted experiments on a dataset from the Mozilla
    Firefox project provide guidelines for applying LDA for bug triaging tasks
    effectively.
  thumbnail: publications/2023-enase-probabilistic-models-extended.png
  downloads:
    - href: 'https://doi.org/10.1007/978-3-031-36597-3_3'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/367351366/inline/jsViewer/64aeb3b4c41fb852dd6c5249?pdfJsDownload=1
      desc: Author Version
  doi: 10.1007/978-3-031-36597-3_3
- key: 2023-evaluating-ssnp
  bibtex: casrd2023-evaluating-ssnp.bib
  date: '2023-02-19'
  title: >-
    Evaluating Architectures and Hyperparameters of Self-supervised Network
    Projections
  authors:
    - Tim Cech
    - Daniel Atzberger
    - Willy Scheibel
    - Rico Richter
    - Jürgen Döllner
  published:
    - venue: 14th International Conference on Information Visualization Theory and Applications
      short: IVAPP '23
      publisher: SciTePress
  doi: 10.5220/0011699700003417
  articletype: Short Paper
  caption: >-
    Self-Supervised Network Projections (SSNP) are dimensionality reduction
    algorithms that produce low-dimensional layouts from high-dimensional
    data. By combining an autoencoder architecture with neighborhood
    information from a clustering algorithm, SSNP intend to learn an embedding
    that generates visually separated clusters. In this work, we extend an
    approach that uses cluster information as pseudo-labels for SSNP by taking
    outlier information into account. Furthermore, we investigate the
    influence of different autoencoders on the quality of the generated
    two-dimensional layouts. We report on two experiments on the autoencoder's
    architecture and hyperparameters, respectively, measuring nine metrics on
    eight labeled datasets from different domains, e.g., Natural Language
    Processing. The results indicate that the model's architecture and the
    choice of hyperparameter values can influence the layout with statistical
    significance, but none achieves the best result over all metrics. In
    addition, we found out that using outlier information for the
    pseudo-labeling approach can maintain global properties of the
    two-dimensional layout while trading-off local properties.
  thumbnail: publications/2023-ivapp-evaluating-ssnp.png
  downloads:
    - href: 'https://doi.org/10.5220/0011699700003417'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/367351449/inline/jsViewer/63f7946257495059453920bf?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/368567010/inline/jsViewer/63ee4afc2958d64a5cd5e389?pdfJsDownload=1
      desc: Slides
    - href: 'https://github.com/Oluwoye/DimensionReduction'
      desc: Github Project
- key: 2023-defect-prediction
  bibtex: casmd2023-defect-prediction.bib
  date: '2023-05-23'
  title: Outlier Mining Techniques for Software Defect Prediction
  authors:
    - Tim Cech
    - Daniel Atzberger
    - Willy Scheibel
    - Sanjay Misra
    - Jürgen Döllner
  published:
    - venue: 15th International Conference on Software Quality
      short: SWQD '23
      publisher: Springer
  doi: 10.1007/978-3-031-31488-9_3
  articletype: Full Paper
  caption: >-
    Software metrics measure aspects related to the quality of software. Using
    software metrics as a method of quantification of software, various
    approaches were proposed for locating defect-prone source code units
    within software projects. Most of these approaches rely on supervised
    learning algorithms, which require labeled data for adjusting their
    parameters during the learning phase. Usually, such labeled training data
    is not available. Unsupervised algorithms do not require training data and
    can therefore help to overcome this limitation. In this work, we evaluate
    the effect of unsupervised learning - especially outlier mining algorithms
    - for the task of defect prediction, i.e., locating defect-prone source
    code units. We investigate the effect of various class balancing and
    feature compressing techniques as preprocessing steps and show how sliding
    windows can be used to capture time series of source code metrics. We
    evaluate the Isolation Forest and Local Outlier Factor, as representants
    of outlier mining techniques. Our experiments on three publicly available
    datasets, containing a total of 11 software projects, indicate that the
    consideration of time series can improve static examinations by up to 3%.
    The results further show that supervised algorithms can outperform
    unsupervised approaches on all projects. Among all unsupervised
    approaches, the Isolation Forest achieves the best accuracy on 10 out of
    11 projects.
  thumbnail: publications/2023-swqd-defect-prediction.png
  downloads:
    - href: 'https://doi.org/10.1007/978-3-031-31488-9_3'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/370746953/inline/jsViewer/64aeb537c41fb852dd6c5365?pdfJsDownload=1
      desc: Author Version
- key: 2023-log-outlier
  bibtex: acsd2023-log-outlier.bib
  date: '2023-04-24'
  title: Detecting Outliers in CI/CD Pipeline Logs using Latent Dirichlet Allocation
  authors:
    - Daniel Atzberger
    - Tim Cech
    - Willy Scheibel
    - Rico Richter
    - Jürgen Döllner
  published:
    - venue: 18th International Conference on Evaluation of Novel Approaches to Software Engineering
      short: ENASE '23
      publisher: SciTePress
  doi: 10.5220/0011858500003464
  articletype: Short Paper
  caption: >-
    Continuous Integration and Continuous Delivery are best practices used
    during the DevOps phase. By using automated pipelines for building and
    testing small software changes, possible risks are intended to be detected
    early. Those pipelines continuously generate log events that are collected
    in semi-structured log files. In an industry context, these log files can
    amass 100,000 events and more. However, the relevant sections in these log
    files must be manually tagged by the user. This paper presents an
    online-learning approach for detecting relevant log events using Latent
    Dirichlet Allocation. After grouping a fixed number of log events in a
    document, our approach prunes the vocabulary to eliminate words without
    semantic meaning. A sequence of documents is then described as a discrete
    sequence by applying Latent Dirichlet Allocation, which allows the
    detection of outliers within the sequence. Our approach provides an
    explanation of the results by integrating the latent variables of the
    model. The approach is tested on log files that originate from a CI/CD
    pipeline of a large German company. Our results indicate that whether or
    not a log event is marked as an outlier heavily depends on the chosen
    hyperparameters of our model.
  thumbnail: publications/2023-enase-log-outlier.png
  downloads:
    - href: 'https://doi.org/10.5220/0011858500003464'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/368566654/inline/jsViewer/6446ab29d749e4340e34c51d?pdfJsDownload=1
      desc: Author Version
- key: 2023-realestatetokenization
  bibtex: hasd2023-realestatetokenization.bib
  date: '2023-05-01'
  title: >-
    Real Estate Tokenization in Germany: Market Analysis and Concept of a
    Regulatory and Technical Solution
  authors:
    - Robert Henker
    - Daniel Atzberger
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 5th International Conference on Blockchain and Cryptocurrency
      short: ICBC '23
      publisher: IEEE
  articletype: Short Paper
  doi: 10.1109/ICBC56567.2023.10174954
  caption: >-
    Real estate is the largest asset class and is equally popular with
    professional and retail investors. However, this asset class has the
    disadvantage that it is very illiquid, and investments have a high entry
    barrier in terms of equity. The adoption of the Electronic Securities Act
    in 2021 by the German Bundestag has created the legal framework for
    tokenizing real estate assets and their management using digital ledger
    technology in Germany. In this paper we describe a business concept for
    managing ownership and business transactions for real estate in Germany
    using blockchain technology. Besides its possibilities, we present a
    market analysis that comprises existing approaches and discusses legal
    limitations specific to Germany.
  thumbnail: publications/2023-icbc-realestatetokenization.png
  downloads:
    - href: 'https://doi.org/10.1109/ICBC56567.2023.10174954'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/369588396/inline/jsViewer/6450f90197449a0e1a7038fa?pdfJsDownload=1
      desc: Author Version
- key: 2023-hephaistos
  bibtex: havsdb2023-hephaistos.bib
  date: '2023-05-01'
  title: >-
    Hephaistos: A Management System for Massive Order Book Data from Multiple
    Centralized Crypto Exchanges with an Internal Unified Order Book
  authors:
    - Robert Henker
    - Daniel Atzberger
    - Jan Ole Vollmer
    - Willy Scheibel
    - Jürgen Döllner
    - Markus Bick
  published:
    - venue: 1st International Workshop on Cryptocurrency Exchanges
      short: CryptoEx '23
      publisher: IEEE
  articletype: Full Paper
  doi: 10.1109/ICBC56567.2023.10174923
  caption: >-
    Offers to buy and sell cryptocurrencies on exchanges are collected in an
    order book as pairs of amount and price provided with a timestamp.
    Contrary to tick data, which only reflects the last transaction price on
    an exchange, the order book reflects the market’s actual price information
    and the available volume. Until now, no system has been presented that can
    capture many different order books across several markets. This paper
    presents Hephaistos, a system for processing, harmonizing, and storing
    massive spot order book data from 22 centralized crypto exchanges and 55
    currency pairs. After collecting the data, Hephaistos aggregates several
    order books in a so-called Unified Order Book, which is the foundation for
    a Smart Order Routing algorithm. As a result an order is splitted across
    several exchanges, which results in a better execution price. As component
    of a high-frequency trading system, Hephaistos captures 32% of the total
    daily spot trading volume. We provide examples with data from two
    exchanges that show that the Smart Order Routing algorithm significantly
    reduces the slippage.
  thumbnail: publications/2023-cryptoex-hephaistos.png
  downloads:
    - href: 'https://doi.org/10.1109/ICBC56567.2023.10174923'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/369588402/inline/jsViewer/645504735762c95ac37645b2?pdfJsDownload=1
      desc: Author Version
- key: 2023-orderbookvis
  bibtex: jahsd2023-orderbookvis.bib
  date: '2023-05-01'
  title: >-
    OrderBookVis: A Visualization Approach for Comparing Order Books from
    Centralized Crypto Exchanges
  authors:
    - Adrian Jobst
    - Daniel Atzberger
    - Robert Henker
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 1st International Workshop on Cryptocurrency Exchanges
      short: CryptoEx '23
      publisher: IEEE
  articletype: Short Paper
  doi: 10.1109/ICBC56567.2023.10174944
  caption: >-
    Trading for a currency pair on centralized crypto exchanges is organized
    via an order book, which collects all open buy and sell orders at any
    given time and thus forms the basis for price formation. Usually, the
    exchanges provide basic visualizations, which show the accumulated buy and
    sell volume in an animated 2D representation. However, this visualization
    does not allow the user to compare different order books, e.g., several
    order book snapshots. In this work, we present OrderBookVis, a 2.5D
    representation that shows a discrete set of order books comparatively. For
    this purpose, the individual snapshots are displayed as a 2D
    representation as usual and placed one after the other on a 2D reference
    plane. As possible use cases, we discuss the analysis of the temporal
    evolution of the order book for a fixed market and the comparison of
    different order books across multiple markets.
  thumbnail: publications/2023-cryptoex-orderbookvis.png
  downloads:
    - href: 'https://doi.org/10.1109/ICBC56567.2023.10174944'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/369588558/inline/jsViewer/645504b297449a0e1a7da052?pdfJsDownload=1
      desc: Author Version
- key: 2023-liquidityanalysis
  bibtex: jahvsd2023-liquidityanalysis.bib
  date: '2023-05-01'
  title: >-
    Examining Liquidity of Exchanges and Assets and the Impact of External
    Events in Centralized Crypto Markets: A 2022 Study
  authors:
    - Adrian Jobst
    - Daniel Atzberger
    - Robert Henker
    - Jan Ole Vollmer
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 1st International Workshop on Cryptocurrency Exchanges
      short: CryptoEx '23
      publisher: IEEE
  articletype: Full Paper
  doi: 10.1109/ICBC56567.2023.10174905
  caption: >-
    Most cryptocurrencies are bought and sold on centralized exchanges that
    manage supply and demand via an order book. Besides trading fees, the high
    liquidity of a market is the most relevant reason for choosing one
    exchange over the other. However, as the different liquidity measures rely
    on the order book, external events that cause people to sell or buy a
    cryptocurrency can significantly impact a market's liquidity. To
    investigate the effect of external events on liquidity, we measure various
    liquidity measures for nine different order books comprising three
    currency pairs across three exchanges covering the entire year 2022. The
    resulting multivariate time series is then analyzed using different
    correlations. From the results, we can infer that as a cryptocurrency's
    market capitalization and the exchange's trading volume increases, so does
    its liquidity. At the same time, only a moderate correlation of liquidity
    between exchanges can be observed. Furthermore, our statistical
    observations show that external events, particularly the events around FTX
    and the Terra Luna crash, caused significant changes in liquidity.
    However, depending on the exchange's size and the cryptocurrency's market
    cap, the liquidity took a shorter or longer time to recover.
  thumbnail: publications/2023-cryptoex-liquidityanalysis.png
  downloads:
    - href: 'https://doi.org/10.1109/ICBC56567.2023.10174905'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/369588456/inline/jsViewer/6455049a97449a0e1a7da04c?pdfJsDownload=1
      desc: Author Version
- key: 2023-saliencnn
  bibtex: cssd2023-saliencnn.bib
  date: '2023-06-12'
  title: >-
    A Dashboard for Interactive Convolutional Neural Network Training And
    Validation Through Saliency Maps
  authors:
    - Tim Cech
    - Furkan Simsek
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 25th {} Conference on Visualization
      short: EuroVis '23
      publisher: EG
  doi: 10.2312/evp.20231054
  articletype: Extended Abstract
  caption: >-
    Quali-quantitative methods provide ways for interrogating Convolutional
    Neural Networks (CNN). For it, we propose a dashboard using a
    quali-quantitative method based on quantitative metrics and saliency maps.
    By those means, a user can discover patterns during the training of a CNN.
    With this, they can adapt the training hyperparameters of the model,
    obtaining a CNN that learned patterns desired by the user. Furthermore,
    they neglect CNNs which learned undesirable patterns. This improves users'
    agency over the model training process.
  thumbnail: publications/2023-eurovis-saliencnn.png
  downloads:
    - href: 'https://doi.org/10.2312/evp.20231054'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/370772987/inline/jsViewer/64aeb586b9ed6874a5153869?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/370773056_Cech_-_EuroVis_2023_Poster_-_A_Dashboard_for_Interactive_Convolutional_Neural_Network_Training_And_Validation_Through_Saliency_Maps_-_Supplemental_Materialpdf/data/6462a5b0fbaf5b27a4cb5399/Cech-EuroVis-2023-Poster-A-Dashboard-for-Interactive-Convolutional-Neural-Network-Training-And-Validation-Through-Saliency-Maps-Supplemental-Material.pdf
      desc: Supplemental Material
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/372307685/inline/jsViewer/64aeb850c41fb852dd6c5641?pdfJsDownload=1
      desc: Poster
- key: 2023-hilbertmoorecontinuous
  bibtex: sd2023-hilbertmoorecontinuous.bib
  date: '2023-06-12'
  title: Constructing Hierarchical Continuity in Hilbert & Moore Treemaps
  authors:
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 25th {} Conference on Visualization
      short: EuroVis '23
      publisher: EG
  doi: 10.2312/evp.20231060
  articletype: Extended Abstract
  caption:
    The Hilbert and Moore treemap layout algorithms are based on the
    space-filling Hilbert and Moore curves, respectively, to map
    tree-structured datasets to a 2D treemap layout. Considering multiple
    snapshots of a time-variant dataset, one of the design goals for Hilbert
    and Moore treemaps is layout stability, i.e., low changes in the layout
    for low changes in the underlying tree-structured data. For this, their
    underlying space-filling curve is expected to be continuous across all
    nodes and hierarchy levels, which has to be considered throughout the
    layouting process. We propose optimizations to subdivision templates,
    their orientation, and discuss the continuity of the underlying
    space-filling curve. We show real-world examples of Hilbert and Moore
    treemaps for small and large datasets with continuous space-filling
    curves, allowing for improved layout stability.
  thumbnail: publications/2023-eurovis-hilbertmoorecontinuous.png
  downloads:
    - href: 'https://doi.org/10.2312/evp.20231060'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/370769807/inline/jsViewer/646246fe434e26474feb0bcd?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/370770050_Scheibel_-_EuroVis_2023_-_Constructing_Hierarchical_Continuity_in_Hilbert_Moore_Treemaps_-_Supplemental_Materialpdf/data/6462477e434e26474feb0bdb/Scheibel-EuroVis-2023-Constructing-Hierarchical-Continuity-in-Hilbert-Moore-Treemaps-Supplemental-Material.pdf
      desc: Supplemental Material
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/370769813/inline/jsViewer/64624808fbaf5b27a4cb4366?pdfJsDownload=1
      desc: Poster
    - href: 'https://doi.org/10.5281/zenodo.7934497'
      desc: Source Code
    - href: 'https://github.com/varg-dev/hilbert-moore-treemap-layouts-prototype'
      desc: Github Project
- key: 2024-evaluation-tm-dr
  bibtex: acstrds2024-evaluation-tm-dr.bib
  date: '2023-10-23'
  title: >-
    Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods
    for 2D Text Spatialization
  authors:
    - Daniel Atzberger
    - Tim Cech
    - Matthias Trapp
    - Rico Richter
    - Willy Scheibel
    - Jürgen Döllner
    - Tobias Schreck
  published:
    - venue: 28th {} Conference on Visualization and Visual Analytics
      short: VIS '23
      publisher: IEEE
    - venue: Transactions on Visualization and Computer Graphics
      short: TVCG
      publisher: IEEE
  articletype: Journal Article
  highlight: true
  pending: false
  doi: 10.1109/TVCG.2023.3326569
  caption: >-
    Topic models are a class of unsupervised learning algorithms for detecting
    the semantic structure within a text corpus. Together with a subsequent
    dimensionality reduction algorithm, topic models can be used for deriving
    spatializations for text corpora as two-dimensional scatter plots,
    reflecting semantic similarity between the documents and supporting corpus
    analysis. Although the choice of the topic model, the dimensionality
    reduction, and their underlying hyperparameters significantly impact the
    resulting layout, it is unknown which particular combinations result in
    high-quality layouts with respect to accuracy and perception metrics. To
    investigate the effectiveness of topic models and dimensionality reduction
    methods for the spatialization of corpora as two-dimensional scatter plots
    (or basis for landscape-type visualizations), we present a large-scale,
    benchmark-based computational evaluation. Our evaluation consists of (1) a
    set of corpora, (2) a set of layout algorithms that are combinations of
    topic models and dimensionality reductions, and (3) quality metrics for
    quantifying the resulting layout. The corpora are given as document-term
    matrices, and each document is assigned to a thematic class. The chosen
    metrics quantify the preservation of local and global properties and the
    perceptual effectiveness of the two-dimensional scatter plots. By
    evaluating the benchmark on a computing cluster, we derived a multivariate
    dataset with over 45000 individual layouts and corresponding quality
    metrics. Based on the results, we propose guidelines for the effective
    design of text spatializations that are based on topic models and
    dimensionality reductions. As a main result, we show that interpretable
    topic models are beneficial for capturing the structure of text corpora.
    We furthermore recommend the use of t-SNE as a subsequent dimensionality
    reduction.
  thumbnail: publications/2024-vis-evaluation-tm-dr.png
  stamps:
    - icon: logos/replicabilitystamp.png
      href: >-
        https://www.replicabilitystamp.org/index.html#https-github-com-hpicgs-topic-models-and-dimensionality-reduction-benchmark
  downloads:
    - href: 'https://doi.org/10.1109/TVCG.2023.3326569'
      desc: Publisher Record
    - href: 'https://doi.org/10.48550/arXiv.2307.11770'
      desc: Preprint
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/374951742/inline/jsViewer/65393bf21d6e8a70704e46ca?pdfJsDownload=1
      desc: Slides
    - href: >-
        https://github.com/hpicgs/Topic-Models-and-Dimensionality-Reduction-Benchmark
      desc: Github Project
    - href: 'https://zenodo.org/record/8113828'
      desc: Pre-trained Topic Models (I)
    - href: 'https://zenodo.org/record/8114601'
      desc: Pre-trained Topic Models (II)
- key: 2023-uncover
  bibtex: lbscsd2023-uncover.bib
  date: '2023-11-13'
  title: >-
    unCover: Identifying AI Generated News Articles by Linguistic Analysis and
    Visualization
  authors:
    - Lucas Liebe
    - Jannis Baum
    - Tilman Schütze
    - Tim Cech
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 15th International Conference on Knowledge Discovery and Information Retrieval
      short: KDIR '23
      publisher: SciTePress
  articletype: Full Paper
  note: Candidate for Best Paper
  pending: false
  doi: 10.5220/0012163300003598
  caption: >-
    Text synthesis tools are becoming increasingly popular and better at
    mimicking human language. In trustsensitive decisions, such as plagiarism
    and fraud detection, identifying AI-generated texts poses larger
    difficulties: decisions need to be made explainable to ensure trust and
    accountability. To support users in identifying AI-generated texts, we
    propose the tool UNCOVER. The tool analyses texts through three
    explainable linguistic approaches: Stylometric writing style analysis,
    topic modeling, and entity recognition. The result of the tool is a
    decision and visualizations on the analysis results. We evaluate the tool
    on news articles by means of accuracy of the decision and an expert study
    with 13 participants. The final prediction is based on classification of
    stylometric and evolving topic analysis. It achieved an accuracy of 70.4%
    and a weighted F1-score of 85.6%. The participants preferred to base their
    assessment on the prediction and the topic graph. However, they found the
    entity recognition to be an ineffective indicator. Moreover, five
    participants highlighted the explainable aspects of UNCOVER and overall
    the participants achieved 69% true classifications. Eight participants
    expressed interest to continue using unCover for identifying AI-generated
    texts.
  thumbnail: publications/2023-kdir-uncover.png
  downloads:
    - href: 'https://doi.org/10.5220/0012163300003598'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/374952434/inline/jsViewer/65605ae63fa26f66f4220126?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/375867257/inline/jsViewer/65605a20ce88b8703107f8c7?pdfJsDownload=1
      desc: Slides
    - href: 'https://uncover.lucasliebe.de/'
      desc: Demo
    - href: 'https://github.com/hpicgs/unCover'
      desc: Github Project
- key: 2023-counterfactuals
  bibtex: btwd2023-counterfactuals.bib
  date: '2023-11-13'
  title: Visual Counterfactual Explanations Using Semantic Part Locations
  authors:
    - Florence Böttger
    - Tim Cech
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 15th International Conference on Knowledge Discovery and Information Retrieval
      short: KDIR '23
      publisher: SciTePress
  articletype: Full Paper
  note: Best Student Paper Award
  pending: false
  doi: 10.5220/0012179000003598
  caption: >-
    As machine learning models are becoming more widespread and see use in
    high-stake decisions, the explainability of these decisions is getting
    more relevant. One approach for explainability are counterfactual
    explanations. They are defined as changes to a data point such that it
    appears as a different class. Their close connection to the original
    dataset aids their explainability. However, existing methods of creating
    counterfacual explanations often rely on other machine learning models,
    which adds an additional layer of opacity to the explanations. We propose
    additions to an established pipeline for creating visual counterfacual
    explanations by using an inherently explainable algorithm that does not
    rely on external models. Using annotated semantic part locations, we
    replace parts of the counterfactual creation process. We evaluate the
    approach on the CUB-200-2011 dataset. Our approach outperforms the
    previous results: we improve (1) the average number of edits by 0.1 edits,
    (2) the key point accuracy of editing within any semantic parts of the
    image by an average of at least 7 percentage points, and (3) the key point
    accuracy of editing the same semantic parts by at least 17 percentage
    points.
  thumbnail: publications/2023-kdir-counterfactuals.png
  downloads:
    - href: 'https://doi.org/10.5220/0012179000003598'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/374952266/inline/jsViewer/65605b81ce88b870310804f1?pdfJsDownload=1
      desc: Author Version
- key: 2024-object-maps
  bibtex: tsd2024-object-maps.bib
  date: '2024-02-27'
  title: >-
    Bringing Objects to Life: Supporting Program Comprehension through Animated
    2.5D Object Maps from Program Traces
  authors:
    - Christoph Thiede
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 15th International Conference on Information Visualization Theory and Applications
      short: IVAPP '23
      publisher: SciTePress
  articletype: Short Paper
  pending: false
  doi: 10.5220/0012393900003660
  caption: >-
    Program comprehension is a key activity in software development. Several
    visualization approaches such as software maps have been proposed to
    support programmers in exploring the architecture of software systems.
    However, for the exploration of program behavior, programmers still rely
    on traditional code browsing and debugging tools to build a mental model
    of a system's behavior. We propose a novel approach to visualizing program
    behavior through animated 2.5D object maps that depict particular objects
    and their interactions from a program trace. We describe our
    implementation and evaluate it for different program traces through an
    experience report and performance measurements. Our results indicate that
    our approach can benefit program comprehension tasks, but further research
    is needed to improve scalability and usability.
  thumbnail: publications/2024-ivapp-object-maps.png
  downloads:
    - href: 10.5220/0012393900003660
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/376650904/inline/jsViewer/65de1e5ec3b52a1170fc2ffe?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Christoph-Thiede/publication/378431164/inline/jsViewer/65de047cc3b52a1170fc1ced?pdfJsDownload=1
      desc: Slides
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/377983193/inline/jsViewer/65c1fe6e34bbff5ba7efa030?pdfJsDownload=1
      desc: Poster
    - href: 'https://linqlover.github.io/trace4d/'
      desc: Prototype
    - href: 'https://github.com/LinqLover/trace4d'
      desc: Github Project
- key: 2024-topic-model-influence
  bibtex: acsds2024-topic-model-influence.bib
  date: '2024-02-27'
  title: >-
    Quantifying Topic Model Influence on Text Layouts based on Dimensionality
    Reductions
  authors:
    - Daniel Atzberger
    - Tim Cech
    - Willy Scheibel
    - Jürgen Döllner
    - Tobias Schreck
  published:
    - venue: 15th International Conference on Information Visualization Theory and Applications
      short: IVAPP '24
      publisher: SciTePress
  articletype: Full Paper
  pending: false
  highlight: true
  note: Best Paper Award
  doi: 10.5220/0012391100003660
  caption: >-
    Text spatializations for text corpora often rely on two-dimensional
    scatter plots generated from topic models and dimensionality reductions.
    Topic models are unsupervised learning algorithms that identify clusters,
    so-called topics, within a corpus, representing the underlying concepts.
    Furthermore, topic models transform documents into vectors, capturing
    their association with topics. A subsequent dimensionality reduction
    creates a two-dimensional scatter plot, illustrating semantic similarity
    between the documents. A recent study by Atzberger et al. has shown that
    topic models are beneficial for generating two-dimensional layouts.
    However, in their study, the hyperparameters of the topic models are
    fixed, and thus the study does not analyze the impact of the topic models'
    quality on the resulting layout. Following the methodology of Atzberger et
    al., we present a comprehensive benchmark comprising (1) text corpora, (2)
    layout algorithms based on topic models and dimensionality reductions, (3)
    quality metrics for assessing topic models, and (4) metrics for evaluating
    two-dimensional layouts' accuracy and cluster separation. Our study
    involves an exhaustive evaluation of numerous parameter configurations,
    yielding a dataset that quantifies the quality of each dataset-layout
    algorithm combination. Through a rigorous analysis of this dataset, we
    derive practical guidelines for effectively employing topic models in text
    spatializations. As a main result, we conclude that the quality of a topic
    model measured by coherence is positively correlated to the layout quality
    in the case of Latent Semantic Indexing and Non-Negative Matrix
    Factorization.
  thumbnail: publications/2024-ivapp-topic-model-influence.png
  downloads:
    - href: 10.5220/0012391100003660
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/378701396/inline/jsViewer/65e85a5bc3b52a11701aca51?pdfJsDownload=1
      desc: Author Version
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/378527886/inline/jsViewer/65df1eeec3b52a1170fc5b1e?pdfJsDownload=1
      desc: Slides
    - href: 'https://doi.org/10.5281/zenodo.10040858'
      desc: Experiments and Results
- key: 2024-githubembedded
  bibtex: sblad2024-githubembedded.bib
  date: '2024-01-17'
  title: Integrated Visual Software Analytics on the GitHub Platform
  authors:
    - Willy Scheibel
    - Jasper Blum
    - Franziska Lauterbach
    - Daniel Atzberger
    - Jürgen Döllner
  published:
    - venue: Computers
      publisher: MDPI
  articletype: Journal Article
  pending: false
  doi: 10.3390/computers13020033
  note: Issue Cover
  highlight: true
  caption: >-
    Readily available software analysis and analytics tools are often operated
    within external services, where the measured software analysis data is
    kept internally and no external access to the data is available. We
    propose an approach to integrate visual software analysis on the GitHub
    platform by leveraging GitHub Actions and the GitHub API, covering both
    analysis and visualization. The process is to perform software analysis
    for each commit, e.g., static source code complexity metrics, and augment
    the commit by the resulting data, stored as git objects within the same
    repository. We show that this approach is feasible by integrating it into
    64 open source TypeScript projects. Further, we analyze the impact on
    Continuous Integration (CI) run time and repository storage. The stored
    software analysis data is externally accessible to allow for visualization
    tools, such as software maps. The effort to integrate our approach is
    limited to enabling the analysis component within the a project's CI on
    GitHub and embed an HTML snippet into the project's website for
    visualization. This enables a large amount of projects to have access to
    software analysis as well as provide means to communicate the current
    status of a project.
  thumbnail: publications/2024-computers-githubembedded.png
  downloads:
    - href: 'https://doi.org/10.3390/computers13020033'
      desc: Publisher Record
    - href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/377444142/inline/jsViewer/65b22be16c7ad06ab428627b?pdfJsDownload=1
      desc: Author Version
    - href: 'https://hpicgs.github.io/github-software-analytics-embedding/'
      desc: Prototype
    - href: 'https://github.com/hpicgs/github-software-analytics-embedding/'
      desc: Github Project
- key: 2024-athena
  bibtex: havsdb2024-athena.bib
  date: '2024-03-27'
  title: >-
    Athena: Smart Order Routing on Centralized Crypto Exchanges using a Unified
    Order Book
  authors:
    - Robert Henker
    - Daniel Atzberger
    - Jan Ole Vollmer
    - Willy Scheibel
    - Jürgen Döllner
    - Markus Bick
  published:
    - venue: International Journal of Network Management
      short: JNEM
      publisher: Wiley
  articletype: Journal Article
  doi: 10.1002/nem.2266
  pending: false
  caption: >-
    Most cryptocurrency spot trading occurs on centralized crypto exchanges,
    where offers for buying and selling are organized via an order book. In
    liquid markets, the price achieved for buying and selling deviates only
    slightly from the assumed reference price, i.e., trading is associated
    with low implicit costs. However, compared to traditional finance crypto
    markets are still illiquid and consequently the reduction of implicit
    costs is crucial for any trading strategy and of high interest, especially
    for institutional investors. This paper describes the design and
    implementation of Athena, a system that automatically splits orders across
    multiple exchanges to minimize implicit costs. For this purpose, order
    books are collected from several centralized crypto exchanges and merged
    into an internal unified order book. In addition to price and quantity,
    the entries in the unified order book are enriched with information about
    the exchange. This enables a smart order routing algorithm to split an
    order into several slices and execute these on several exchanges to reduce
    implicit costs and achieve a better price.
  thumbnail: publications/2024-jnem-athena.png
  downloads:
    - desc: Publisher Record
      href: 'https://doi.org/10.1002/nem.2266'
    - desc: Author Version
      href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/379309357/inline/jsViewer/6603d7fb49de030f4cc95436?pdfJsDownload=1
    - href: 'https://doi.org/10.5281/zenodo.10600373'
      desc: Data
- key: 2024-perpetual-swaps
  bibtex: amhsdk2024-perpetual-swaps.bib
  date: '2024-05-31'
  title: A Low-Volatility Strategy based on Hedging a Quanto Perpetual Swap on BitMEX
  authors:
    - Daniel Atzberger
    - Toshiko Matsui
    - Robert Henker
    - Willy Scheibel
    - Jürgen Döllner
    - William Knottenbelt
  published:
    - venue: 2nd International Workshop on Cryptocurrency Exchanges
      short: CryptoEx '24
      publisher: IEEE
  articletype: Short Paper
  doi: 10.1109/ICBC59979.2024.10634346
  pending: false
  caption: >-
    In 2016, BitMEX introduced a novel type of crypto derivates: Perpetual
    Swaps, i.e., futures with an infinite term. Perpetual swaps provide a new
    strategic risk management tool for cryptocurrencies due to their
    custody-free nature, high leverage, and funding mechanism, but there has
    been little quantitative analysis on the their benefits. In this paper, we
    introduce a trading strategy that combines a Quanto Perpetual Swap with a
    spot position to benefit from the funding mechanism. We compare our
    strategy with a long-only investment in the underlying cryptocurrency and
    a similar strategy based on Linear Perpetual Swaps to evaluate their
    performances in a large-scale backtest covering the years 2021 and 2022.
    Our analysis shows that our strategy generates positive returns in bullish
    market phases of the underlying with lower volatility.
  thumbnail: publications/2024-cryptoex-perpetual-swaps.png
  downloads:
    - desc: Publisher Record
      href: 'https://doi.org/10.1109/ICBC59979.2024.10634346'
    - desc: Author Version
- key: 2024-fisc
  bibtex: cksd2024-fisc.bib
  date: '2024-05-28'
  title: >-
    A Dashboard for Simplifying Machine Learning Models using Feature
    Importances and Spurious Correlation Analysis
  authors:
    - Tim Cech
    - Erik Kohlros
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 26th {} Conference on Visualization
      short: EuroVis '24
      publisher: EG
  articletype: Extended Abstract
  pending: false
  doi: 10.2312/evp.20241075
  caption: >-
    Machine Learning models underlie a trade-off between accurracy and
    explainability. Given a trained, complex model, we contribute a dashboard
    that supports the process to derive more explainable models, here:
    Fast-and-Frugal Trees, with further introspection using feature
    importances and spurious correlation analyses. The dashboard further
    allows to iterate over the feature selection and assess the trees'
    performance in comparison to the complex model.
  thumbnail: publications/2024-eurovis-fisc.png
  downloads:
    - desc: Publisher Record
      href: 'https://doi.org/10.2312/evp.20241075'
    - desc: Author Version
      href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/380076157/inline/jsViewer/662a47709d2a69723f6483a0?pdfJsDownload=1
    - desc: Supplemental Material
      href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/380076439/data/662a481506ea3d0b740baa5a/Cech-2024-FISC-Supplemental-Material.pdf?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6ImhvbWUiLCJwYWdlIjoicHVibGljYXRpb24iLCJwb3NpdGlvbiI6InBhZ2VDb250ZW50In19
    - desc: Poster
      href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/380076169/inline/jsViewer/662a492708aa54017abf751d?pdfJsDownload=1
- key: 2024-lmds
  bibtex: crssd2024-lmds.bib
  date: '2024-05-28'
  title: Interactive Human-guided Dimensionality Reduction using Landmark Positioning
  authors:
    - Tim Cech
    - Christian Raue
    - Frederic Sadrieh
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 26th {} Conference on Visualization
      short: EuroVis '24
      publisher: EG
  articletype: Extended Abstract
  pending: false
  doi: 10.2312/evp.20241085
  caption: >-
    Dimensionality Reduction (DR) techniques are used for projecting
    high-dimensional data onto a two-dimensional plane. One subclass of DR
    techniques are such techniques that utilize landmarks. Landmarks are a
    subset of the original data space that are projected by a slow and more
    precise technique. The other data points are then placed in relation to
    these landmarks with respect to their distance in the high-dimensional
    space. We propose a technique to refine the placement of the landmarks by
    a human user. We test two different techniques for unprojecting the
    movement of the low-dimensional landmarks into the high-dimensional data
    space. We showcase that such a movement can increase certain quality
    metrics while decreasing others. Therefore, users may use our technique to
    challenge their understanding of the high-dimensional data space.
  thumbnail: publications/2024-eurovis-lmds.png
  downloads:
    - desc: Publisher Record
      href: 'https://doi.org/10.2312/evp.20241085'
    - desc: Author Version
      href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/380076459/inline/jsViewer/662a4a929d2a69723f64853d?pdfJsDownload=1
    - desc: Supplemental Material
      href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/380094325/data/662a55e506ea3d0b740bb695/Cech-2024-LMDS-Supplemental-Material.pdf?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6ImhvbWUiLCJwYWdlIjoicHVibGljYXRpb24iLCJwb3NpdGlvbiI6InBhZ2VDb250ZW50In19
    - desc: Poster
      href: >-
        https://www.researchgate.net/profile/Willy-Scheibel/publication/380094475/inline/jsViewer/662a546706ea3d0b740bb4bd?pdfJsDownload=1
- key: 2024-point-cloud-npr
  bibtex: wstrd2024-point-cloud-npr.bib
  date: '2024-05-07'
  title: >-
    A Survey on Non-photorealistic Rendering Approaches for Point Cloud
    Visualization
  authors:
    - Ole Wegen
    - Willy Scheibel
    - Matthias Trapp
    - Rico Richter
    - Jürgen Döllner
  published:
    - venue: Transactions on Visualization and Computer Graphics
      short: TVCG
      publisher: IEEE
    - venue: 29th {} Conference on Visualization and Visual Analytics
      short: VIS '24
      publisher: IEEE
  articletype: Journal Article
  doi: 10.1109/TVCG.2024.3402610
  highlight: true
  caption: >-
    Point clouds are widely used as a versatile representation of 3D entities
    and scenes for all scale domains and in a variety of application areas,
    serving as a fundamental data category to directly convey spatial
    features. However, due to point sparsity, lack of structure, irregular
    distribution, and acquisition-related inaccuracies, results of point cloud
    visualization are often subject to visual complexity and ambiguity. In
    this regard, non-photorealistic rendering can improve visual communication
    by reducing the cognitive effort required to understand an image or scene
    and by directing attention to important features. In the last 20 years,
    this has been demonstrated by various non-photorealistic rendering
    approaches that were proposed to target point clouds specifically.
    However, they do not use a common language or structure for assessment
    which complicates comparison and selection. Further, recent developments
    regarding point cloud characteristics and processing, such as massive data
    size or web-based rendering are rarely considered. To address these
    issues, we present a survey on non-photorealistic rendering approaches for
    point cloud visualization, providing an overview of the current state of
    research. We derive a structure for the assessment of approaches,
    proposing seven primary dimensions for the categorization regarding
    intended goals, data requirements, used techniques, and mode of operation.
    We then systematically assess corresponding approaches and utilize this
    classification to identify trends and research gaps, motivating future
    research in the development of effective non-photorealistic point cloud
    rendering methods.
  thumbnail: publications/2024-tvcg-point-cloud-npr.png
  downloads:
    - desc: Publisher Record
      href: 'https://doi.org/10.1109/TVCG.2024.3402610'
    - desc: Author Version
      href: >-
        https://www.researchgate.net/profile/Ole-Wegen/publication/380937598/inline/jsViewer/6656d7040b0d28457461c8b6?pdfJsDownload=1
    - desc: Slides
      href: >-
        https://www.researchgate.net/profile/Ole-Wegen/publication/385851136/inline/jsViewer/673741e168de5e5a3077e6c9?pdfJsDownload=1


- key: 2024-standardness-clouds-meaning
  bibtex: cwarsd2024-standardness-clouds-meaning.bib
  date: '2024-06-21'
  title: >-
    Standardness Clouds Meaning: A Position Regarding the Informed Usage of
    Standard Datasets
  authors:
    - Tim Cech
    - Ole Wegen
    - Daniel Atzberger
    - Rico Richter
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: Machine Learning (cs.LG); Human-Computer Interaction (cs.HC)
      publisher: arXiv CoRR
  articletype: Preprint
  doi: 10.48550/arXiv.2406.13552
  caption: >-
    Standard datasets are frequently used to train and evaluate Machine
    Learning models. However, the assumed standardness of these datasets leads
    to a lack of in-depth discussion on how their labels match the derived
    categories for the respective use case. In other words, the standardness
    of the datasets seems to fog coherency and applicability, thus impeding
    the trust in Machine Learning models. We propose to adopt Grounded Theory
    and Hypotheses Testing through Visualization as methods to evaluate the
    match between use case, derived categories, and labels of standard
    datasets. To showcase the approach, we apply it to the 20 Newsgroups
    dataset and the MNIST dataset. For the 20 Newsgroups dataset, we
    demonstrate that the labels are imprecise. Therefore, we argue that
    neither a Machine Learning model can learn a meaningful abstraction of
    derived categories nor one can draw conclusions from achieving high
    accuracy. For the MNIST dataset, we demonstrate how the labels can be
    confirmed to be defined well. We conclude that a concept of standardness
    of a dataset implies that there is a match between use case, derived
    categories, and class labels, as in the case of the MNIST dataset. We
    argue that this is necessary to learn a meaningful abstraction and, thus,
    improve trust in the Machine Learning model.
  thumbnail: publications/2024-arxiv-standardness-fogs-meaning.png
  downloads:
    - desc: Preprint Record
      href: 'https://doi.org/10.48550/arXiv.2406.13552'
    - desc: Author Version
      href: >-
        https://www.researchgate.net/publication/381580013/inline/jsViewer/6674f3ddd21e220d89c51365?pdfJsDownload=1


- key: 2024-uncover-extended
  bibtex: lbcsd2024-uncover-extended.bib
  date: '2024-08-01'
  title: >-
    Detecting and Comparing LLM Capabilities to Human Writers through Linguistic Analysis
  authors:
    - Lucas Liebe
    - Jannis Baum
    - Tim Cech
    - Willy Scheibel
    - Jürgen Döllner
  published: Preprint
  articletype: Full Paper
  pending: true
  disabled: true
  caption: >-
    The capabilities of Large Language Models (LLMs) to synthesize texts that
    imitate human language have increased rapidly. While many people adopt
    this technology, the potential harm caused through texts synthesized by
    machines is not fully assessed. Factual errors due to model hallucinations
    are especially impactful in media like news articles, which serves an
    important function in society. Therefore, users require support in
    detecting LLM-generation to decrease risks posed by machine text
    synthesis. For this purpose, we propose the tool unCover based on
    explainable linguistic analysis. The tools analyzes texts through
    stylometric writing style analysis for grammatical information and topic
    modeling for semantic information. Its stylometry is based on character,
    word and syntactic trigrams. By inspecting the proposed techniques, the
    differences of LLM-generation can be uncovered. These findings are used to
    explain how text synthesis can be detected. unCovers result is presented
    as a classification and visualization of the analysis. The final
    classification achieved an accuracy of 77.56% and a weighted F1-score of
    86.51%. This is comparable to state-of-the-art products that detect LLMs
    while remaining technically explainable. German news articles are
    classified 1with an accuracy of 66.45%. The visualization supports the
    decision of the tool and can help users navigate complex texts. Through
    the promising results, unCover addresses challenges posed by AI content
    with new solutions. This is a step towards safely integrating LLMs into
    various areas of society.
  thumbnail: publications/2024-sncs-uncover-extended.png
  downloads:
    - desc: Publisher Record
    - desc: Author Version
- key: 2024-counterfactuals-extended
  bibtex: btwd2024-counterfactuals-extended.bib
  date: '2024-08-01'
  title: >-
    An Approach and Evaluation of Visual Counterfactual Explanations Using Semantic Parts Metadata
  authors:
    - Florence Böttger
    - Tim Cech
    - Willy Scheibel
    - Jürgen Döllner
  published: Preprint
  articletype: Full Paper
  pending: true
  disabled: true
  caption: >-
    Counterfactual explanations are widely used to explain black-box models,
    with some approaches generating counterfactuals as a set of edits between
    two images. However, such explanations often do not guarantee coherence
    between the edits. We expand on an existing counterfactual creation
    approach by using semantic parts metadata that contains the locations of
    keypoints in the image. Using this metadata, we encourage counterfactuals
    that edit a given semantic part with other instances of that part. In a
    series of computational experiments on the CUB-200-2011 dataset, we
    determine that the proposed approach (1) decreases the number of edits by
    0.1, (2) increases the keypoint accuracy of editing between two keypoints
    by at least 7.3 pp, and (3) increases the keypoint accuracy of editing
    between the same keypoint by at least 17.7 pp. In order to evaluate the
    impact of the generated explanations on human perception, we perform a
    user study where participants are aided by the explanations in a task of
    distinguishing two classes. This user study found that the proposed
    approach performs worse than the existing one w.r.t. human perception,
    with a loss in accuracy of at least 6.7 pp and up to 31.3 pp. Based on the
    mismatch between the computational experiments and the user study's
    results, we present a critical discussion on the choice of metrics as well
    as the design of the experiments.
  thumbnail: publications/2024-sncs-counterfactuals-extended.png
  downloads:
    - desc: Publisher Record
    - desc: Author Version

- key: 2024-instanced-glyph-rendering
  bibtex: sasd2024-instanced-glyph-rendering.bib
  title: >-
    Instanced Rendering of Parameterized 3D Glyphs with Adaptive Level-of-Detail
    using three.js
  date: '2024-09-25'
  authors:
    - Sandro Steeger
    - Daniel Atzberger
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 29th International Conference on 3D Web Technology
      short: Web3D '24
      publisher: ACM
  articletype: Full Paper
  doi: 10.1145/3665318.3677171
  pending: false
  caption: >-
    This paper contributes an optimized web-based rendering approach and
    implementation for parameterized meshes used as 3D glyphs for information
    visualization. The approach is based on geometry instancing in three.js
    with dynamic mesh selection and data-driven parameterization of the
    meshes. As an application example, we demonstrate a visualization
    prototype of a 2.5D information landscape that allows for exploration of
    source code modules of a software system. To this end, each data point is
    represented by a 3Dglyph selected from a glyph atlas according to its type
    and level-of-detail. We benchmark the approach against a straight-forward
    baseline implementation using regular three.js meshes by evaluating the
    overall run-time performance. For this, we used real-world datasets and
    synthetic larger variants with up to 50000 data points. The proposed
    approach achieves up to a 3000% higher median FPS count on laptop and
    desktop-class hardware and allows us to visualize up to 1 300% larger
    datasets interactively compared to the baseline implementation. This
    indicates that instanced rendering of parameterized meshes allows to
    provide interactive visualization using 3D glyphs for datasets of the next
    order of magnitude.
  thumbnail: publications/2024-web3d-instanced-glyph-rendering.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.1145/3665318.3677171
    - desc: Author Version
    - desc: Slides
    - desc: Demo
      href: https://varg-dev.github.io/SoftwareMap/
    - desc: Github Project
      href: https://github.com/varg-dev/SoftwareMap
    - desc: Performance Measurements
      href: https://doi.org/10.5281/zenodo.12797370

- key: 2024-scatterplot-filtering
  bibtex: ajsd2024-scatterplot-filtering.bib
  title: >-
    Exploring High-Dimensional Data by Pointwise Filtering of Low-Dimensional Embeddings
  date: '2024-09-12'
  authors:
    - Daniel Atzberger
    - Adrian Jobst
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 42nd Conference on Computer Graphics & Visual Computing
      short: CGVC '24
      publisher: EG
  articletype: Short Paper
  doi: 10.2312/cgvc.20241224
  pending: false
  caption: >-
    Dimensionality reductions are a class of unsupervised learning algorithms that aim
    to find a lower-dimensional embedding for a high-dimensional dataset while preserving
    local and global structures. By representing a high-dimensional dataset as a
    two-dimensional scatterplot, a user can explore structures within the dataset.
    However, dimensionality reductions inherit distortions that might result in false
    deductions. This work presents a visualization approach that combines a
    two-dimensional scatterplot derived from a dimensionality reduction with two pointwise
    filtering possibilities. Each point is associated with two pointwise metrics that
    quantify the correctness of its neighborhood and similarity to surrounding data points.
    By setting threshold for these two metrics, the user is supported in several
    scatterplot analytics tasks, e.g., class separation and outlier detection. We apply
    our visualization to a text corpus to detect interesting data points visually and
    discuss the findings.
  thumbnail: publications/2024-cgvc-scatterplot-filtering.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.2312/cgvc.20241224
    - desc: Author Version
    - desc: Slides


- key: 2025-dr-sensitivity-analysis
  bibtex: acsdbs2025-dr-sensitivity-analysis.bib
  title: >-
    A Large-Scale Sensitivity Analysis on Latent Embeddings and Dimensionality
    Reductions for Text Spatializations
  date: '2024-09-17'
  authors:
    - Daniel Atzberger
    - Tim Cech
    - Willy Scheibel
    - Jürgen Döllner
    - Michael Behrisch
    - Tobias Schreck
  published:
    - venue: 29th {} Conference on Visualization and Visual Analytics
      short: VIS '24
      publisher: IEEE
    - venue: Transactions on Visualization and Computer Graphics
      short: TVCG
      publisher: IEEE
  articletype: Journal Article
  doi: 10.1109/TVCG.2024.3456308
  highlight: true
  pending: false
  caption: >-
    The semantic similarity between documents of a text corpus can be visualized using
    map-like metaphors based on two-dimensional scatterplot layouts. These layouts result
    from a dimensionality reduction on the document-term matrix or a representation within
    a latent embedding, including topic models. Thereby, the resulting layout depends on
    the input data and hyperparameters of the dimensionality reduction and is therefore
    affected by changes in them. Furthermore, the resulting layout is affected by changes
    in the input data and hyperparameters of the dimensionality reduction. However, such
    changes to the layout require additional cognitive efforts from the user. In this work,
    we present a sensitivity study that analyzes the stability of these layouts concerning
    (1) changes in the text corpora, (2) changes in the hyperparameter, and (3) randomness
    in the initialization. Our approach has two stages: data measurement and data analysis.
    First, we derived layouts for the combination of three text corpora and six text
    embeddings and a grid-search-inspired hyperparameter selection of the dimensionality
    reductions. Afterward, we quantified the similarity of the layouts through ten metrics,
    concerning local and global structures and class separation. Second, we analyzed the
    resulting 42817 tabular data points in a descriptive statistical analysis. From this, we
    derived guidelines for informed decisions on the layout algorithm and highlight specific
    hyperparameter settings. We provide our implementation as a Git repository at
    https://github.com/hpicgs/Topic-Models-and-Dimensionality-Reduction-Sensitivity-Study and
    results as Zenodo archive at https://doi.org/10.5281/zenodo.12772898.
  thumbnail: publications/2025-vis-stability.png
  downloads:
    - desc: Publisher Record
      href: htts://doi.org/10.1109/TVCG.2024.3456308
    - desc: Preprint
      href: https://doi.org/10.48550/arXiv.2407.17876
    - desc: Slides
    - desc: Github Project
      href: >-
        https://github.com/hpicgs/Topic-Models-and-Dimensionality-Reduction-Sensitivity-Study
    - desc: Results
      href: https://doi.org/10.5281/zenodo.12772898

- key: 2026-topic-model-influence-extended
  bibtex: atzberger2026-topic-model-influence-extended.bib
  date: '2025-06-18'
  title: >-
    Evaluating Text Embeddings for Two-Dimensional Text Corpora Representations
  authors:
    - Daniel Atzberger
    - Tim Barz-Cech
    - Willy Scheibel
    - Jürgen Döllner
    - Tobias Schreck
  published:
    - venue: '{} Information Visualization'
      short: IVI
      publisher: SAGE
  doi: 10.1177/14738716251355650
  articletype: Journal Article
  pending: false
  highlight: true
  note: Issue Cover
  caption: >-
    Several text corpus visualizations utilize a map-like metaphor, where the layout reflects the semantic similarity between
    documents. The underlying two-dimensional scatterplots are created by combining a latent embedding with a subsequent
    dimensionality reduction. In this work, we analyze the impact of embedding quality on layout quality. We evaluate the
    accuracy of the layout, specifically the preservation of local and global structures of the text corpus in its two-dimensional
    representation. Additionally, we assess class separation, focusing on the effectiveness of distinguishing classes within
    the two-dimensional space. We introduce a benchmark B = (D, L, QE , QDR) consisting of a collection of text corpora
    D, a set of layout algorithms L that combine text embeddings with dimensionality reductions, quality metrics QE for
    evaluating text embeddings, and quality metrics QDR for assessing accuracy and class separation. We generate a
    multivariate dataset by evaluating this benchmark, which we further analyze in a descriptive analysis. Our
    results indicate that, for Latent Semantic Indexing combined with tf-idf weighting and t-distributed Stochastic Neighbor
    Embedding, coherence plays a substantial role in determining the accuracy of the layout. Additionally, our
    findings reveal that embeddings do not enhance class separation in the two-dimensional scatterplot representation. As
    main result, we provide more fine-grained guidelines for effectively utilizing text embeddings and dimensionality reduction
    techniques to generate two-dimensional scatterplot representations of text corpora reflecting semantic similarity.
  thumbnail: publications/2025-ivi-topic-model-influence-extended.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.1177/14738716251355650
    - desc: Author Version

- key: 2025-llm-treemaps
  bibtex: jasds2025-llm-treemaps.bib
  date: '2025-02-26'
  title: "Delphi: A Natural Language Interface for 2.5D Treemap Visualization of Source Code"
  authors:
    - Adrian Jobst
    - Daniel Atzberger
    - Willy Scheibel
    - Jürgen Döllner
    - Tobias Schreck
  published:
    - venue: 16th International Conference on Information Visualization Theory and Applications
      short: IVAPP '25
      publisher: SciTePress
  articletype: Short Paper
  pending: false
  doi: 10.5220/0013119600003912
  highlight: false
  caption: >-
    Modern software development projects are characterized by large teams of developers, diverse technology
    stacks, and systematic workflows. This inherent complexity makes it difficult for stakeholders to maintain an
    overview of the project and its evolution. Software Visualization concerns generating data-driven geometric
    representations of specific aspects of software systems to provide insights and enable exploration. However,
    effective utilization of these specialized visualizations requires expertise in visualization theory and software
    development. This paper presents Delphi, the first system that combines a Natural Language Interface backed
    by a Large Language Model with a 2.5D treemap as software visualization technique. Delphi modifies the
    visual mapping to answer questions related to the software project, highlights objects, and provides explanations
    for the user. We demonstrate our system's workflow through a use case study involving a mid-sized TypeScript
    project, showing how Delphi facilitates exploration. Our findings indicate that Delphi enhances the exploration
    process's efficiency and broadens accessibility for a wider range of users. We release our source code as open
    source at https://github.com/hpicgs/llm-treemaps, with our prototype hosted on https://hpicgs.github.io/llm-treemaps.
  thumbnail: publications/2025-ivapp-llm-treemaps.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.5220/0013119600003912
    - desc: Author Version
    - desc: Poster

- key: 2025-apchis-lod
  bibtex: tbajsls2025-apchis-lod.bib
  date: '2025-03-24'
  title: "A User-Centric Adaption Model for Document Visualizations with Different Levels of Detail within a Consumer Health Information System"
  authors:
    - Mariia Tytarenko
    - Christian Wolfgang Burtscher
    - Daniel Atzberger
    - Adrian Jobst
    - Willy Scheibel
    - Stefan Lengauer
    - Tobias Schreck
  published:
    - venue: 1st Workshop on Intelligent and Interactive Health User Interfaces
      short: HealthIUI '25
      publisher: CEUR-WS
  articletype: Short Paper
  pending: false
  highlight: false
  caption: >-
    After receiving a diagnosis, patients often seek detailed information about causes, treatments, and consequences through health information materials such as websites and brochures.
    However, navigating these resources and finding specific information on their diagnosis and ways to respond, can be time-consuming and challenging.
    Text visualizations can help users access relevant information, but typically require knowledge of health literacy and visualization concepts.
    To address this, we propose a user-centric adaptation model for visualizations with varying Levels of Detail (LoD) in healt-related text visualizations to enhance users' ability to explore health information materials.
    The initial LoD is personalized based on a user's visualization literacy, assessed during and onboarding phase. Heuristics derived from interaction metrics, such as dwelling time and mouse clicks, guide subsequent LoD adaptations, enabling the system to respond dynamically to user behavior.
    This combined approach balances user-driven customization with system-driven adaptability, potentially improving engagement, accessibility, and comprehension.
    Our work establishes a foundation for future research on adaptive LoD visualizations tailored to individual abilities and preferences in health information systems.
  thumbnail: publications/2025-healthiui-apchis-lod.png
  downloads:
    - desc: Publisher Record
      href: https://ceur-ws.org/Vol-3957/HealthIUI-paper13.pdf

- key: 2025-llm-treemaps-sensitivity
  bibtex: ajtsds2025-llm-treemaps-sensitivity.bib
  date: '2025-04-28'
  title: "Analyzing the Sensitivity of Prompt Engineering Techniques in Natural Language Interfaces for 2.5D Software Visualization"
  authors:
    - Daniel Atzberger
    - Adrian Jobst
    - Mariia Tytarenko
    - Willy Scheibel
    - Jürgen Döllner
    - Tobias Schreck
  published:
    - venue: 2nd Workshop on Prompt Engineering for Pre-Trained Language Models
      short: PromptEng '25
      publisher: ACM
  articletype: Short Paper
  doi: '10.1145/3701716.3717813'
  pending: false
  highlight: false
  caption: >-
    Natural Language Interfaces (NLIs) backed by Large Language Models (LLMs) are used to interact with visualizations through natural language queries.
    Using the specific example of 2.5D treemaps, the Delphi tool was recently presented, introducing an interactive 2.5D visualization with an accompanying chat interface, where the LLM can react to user input and adapt the visualization at its own discretion.
    While Delphi has demonstrated effectiveness, the authors have not included an evaluation of the LLM's performance with respect to its prompt and specific task types.
    In this study, we systematically evaluate the impact of prompt engineering on Delphi's ability to answer factual questions related to data and visualization.
    Specifically, we investigate the effect of the Chain-of-Thought prompting technique by employing a questionnaire comprising 40 questions across ten low-level analytic tasks.
    Our findings aim to refine prompt design methodologies and enhance the usability and effectiveness of NLIs in advanced visualization systems.
  thumbnail: publications/2025-prompteng-llm-treemaps-sensitivity.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.1145/3701716.3717813
    - desc: Author Version
    - desc: Slides

- key: 2025-llm-vis-grammars
  bibtex: jatsds2025-llm-vis-grammars.bib
  date: '2025-04-28'
  title: "A Concept for Integrating an LLM-Based Natural Language Interface for Visualizations Grammars"
  authors:
    - Adrian Jobst
    - Daniel Atzberger
    - Mariia Tytarenko
    - Willy Scheibel
    - Jürgen Döllner
    - Tobias Schreck
  published:
    - venue: 2nd Workshop on Prompt Engineering for Pre-Trained Language Models
      short: PromptEng '25
      publisher: ACM
  articletype: Extended Abstract
  doi: '10.1145/3701716.3717812'
  pending: false
  highlight: false
  caption: >-
    In this paper, we propose a natural language interface visualization framework leveraging visualization grammar to balance the flexibility and stability of generated visualizations.
    Our system employs a JSON schema for visualization specification and an instruction prompt with semantically distinct sections for task context, visualizations, datasets, and control mechanisms.
    This design enables robust state management, live prompt adjustments, ensures clarity, consistency, and reusability in visualization generation.
  thumbnail: publications/2025-prompteng-llm-vis-grammars.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.1145/3701716.3717812
    - desc: Author Version
    - desc: Slides

- key: 2025-option-tick-data-deribit
  bibtex: ahhvsd2025-option-tick-data-deribit.bib
  date: '2025-06-05'
  title: "A Low-Latency System for Collecting Massive Crypto Option Tick Data from Deribit"
  authors:
    - Daniel Atzberger
    - Robert Henker
    - André Holdschick
    - Jan Ole Vollmer
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 8th Crypto Valley Conference
      short: CVC '25
      publisher: IEEE
  articletype: Short Paper
  doi: 10.1109/CVC65719.2025.00013
  pending: false
  highlight: false
  caption: >-
    As cryptocurrency markets mature, options on cryptocurrencies are gaining increasing importance.
    Deribit has emerged as the dominant exchange for crypto options, accounting for the majority
    of trading volume. To facilitate backtesting and market analysis, access to high-quality
    historical data is essential. In this paper, we introduce a low-latency system for collecting
    massive volumes of tick data together with the best bid and best ask from Deribit. Tick data,
    which capture every price change with precise timestamps, provide granular insights into market
    microstructure and trading dynamics. We detail the technical aspects of our data collection system
    and demonstrate its capabilities through a case study analyzing the implied volatility and bid-ask
    spreads of Bitcoin/USD options around the 2024 U.S. presidential election.
  thumbnail: publications/2025-cvc-option-tick-data-deribit.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.1109/CVC65719.2025.00013
    - desc: Author Version

- key: 2025-task-taxonomies-dust-and-magnet
  bibtex: bsd2025-task-taxonomies-dust-and-magnet.bib
  date: '2025-06-03'
  title: Discussion and Showcase of an Implementation of Task Taxonomies in Visualizations Based on the Dust and Magnet Metaphor
  authors:
    - Florence Böttger
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 27th {} Conference on Visualization
      short: EuroVis '25
      publisher: EG
  articletype: Extended Abstract
  doi: '10.2312/evp.20251140'
  highlight: false
  caption: >-
    In order to visualize multivariate data, it is often necessary to reduce their dimensionality. Visualizations based on
    the Dust and Magnet metaphor aim to present the information of the reduced dimensions via interactivity, while also
    supporting user comprehension by using a metaphor of ferrous dust and magnets. We compare two approaches to implementing
    this metaphor into a visualization tool along a set of core visualization tasks and propose extensions to make such
    visualizations more comprehensible and less misleading.
  thumbnail: publications/2025-eurovis-task-taxonomies-dust-and-magnet.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.2312/evp.20251140
    - desc: Author Version
    - desc: Poster

- key: 2025-graph-evaluation-system
  bibtex: ssd2025-graph-evaluation-system.bib
  date: '2025-06-03'
  title: A Graph Layout Evaluation System for Communication Graphs
  authors:
    - Valentin Schröter
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 27th {} Conference on Visualization
      short: EuroVis '25
      publisher: EG
  articletype: Extended Abstract
  doi: '10.2312/evp.20251138'
  highlight: false
  caption: >-
    Communication networks are a type of graph that occurs in different areas such as robotic, Internet of Things, and
    general network communication. Layouting such networks for visualization is an ongoing research task, while already
    existing layout algorithms are plenty and allow for broad parameterization, making the choice of a fitting algorithm
    difficult. We propose a graph layout evaluation system where a user -- a researcher or visualization designer -- can
    upload own graphs, select pre-loaded ones, or generate synthetic graphs to explore different layouts through their
    generated layouts. The system allows for configuration of multiple layouts that can then be explored by the user
    using side-by-side comparison and an enlarged view. Further, the system allows for computation of layout metrics.
    The overview of different layouts and associated layout metrics can then be used to select a fitting algorithm. The
    system is prepared for further layout algorithms and layout quality metrics, building a starting point for graph
    layout research.
  thumbnail: publications/2025-eurovis-graph-evaluation-system.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.2312/evp.20251138
    - desc: Author Version
    - desc: Poster

- key: 2025-llm-evaluation-system
  bibtex: jasd2025-llm-evaluation-system.bib
  date: '2025-06-03'
  title: Towards a Software Framework for Evaluating the Visualization Literacy of Large Language Models
  authors:
    - Adrian Jobst
    - Daniel Atzberger
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 27th {} Conference on Visualization
      short: EuroVis '25
      publisher: EG
  articletype: Extended Abstract
  doi: '10.2312/evp.20251134'
  highlight: false
  caption: >-
    Large Language Models (LLMs) are increasingly integrated into Natural Language Interfaces (NLIs)
    for visualizations, enabling users to inquire about visualizations through natural language. This
    work introduces a software framework for evaluating LLMs' visualization literacy, i.e., their
    ability to interpret and answer questions about visualizations. Our framework generates a set of data
    points across different LLMs, prompts, and question types, allowing for in-depth analysis. We
    demonstrate its utility by two experiments, examining the impact of the temperature parameter and
    predefined answer choices.
  thumbnail: publications/2025-eurovis-llm-evaluation-system.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.2312/evp.20251134
    - desc: Author Version
    - desc: Poster

- key: 2025-column-based-parsing
  bibtex: sd2025-column-based-parsing.bib
  title: >-
    Parallelized Tabular Data Loading using Web Workers for Hardware-accelerated Visualization Rendering
  date: '2025-09-08'
  authors:
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 30th International Conference on 3D Web Technology
      short: Web3D '25
      publisher: ACM
  articletype: Full Paper
  doi: 10.1145/3746237.3746306
  pending: false
  caption: >-
    Interactive web-based 3D visualization of large tabular datasets requires efficient loading to ensure
    quick access, responsiveness, and interactivity. Current web technologies, in particular JavaScript-based
    CSV parsers, face significant performance bottlenecks due to JavaScript's inherent single-threaded nature
    and inefficient data handling mechanisms. To overcome these limitations, this paper presents a
    multithreaded loading and parsing approach for tabular data leveraging Web Workers and the Streams API.
    Our method partitions input data into manageable batches to be processed in parallel, significantly
    reducing parsing time. Furthermore, the parsed data is directly stored in ArrayBuffers, enabling direct
    and efficient transfer to GPU memory using WebGL or WebGPU, which is critical for visualizing large
    datasets. We evaluate our parser against state-of-the-art parsers, demonstrating substantial performance
    gains -- specifically, parsing a dataset of 10 million rows significantly faster than the fastest
    state-of-the-art parsers for the web. Our approach can be applied as a robust and scalable solution
    tailored for real-time, web-based 3D information and data visualization such as scatter plots, treemaps,
    and information landscapes.
  thumbnail: publications/2025-web3d-column-based-parsing.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.1145/3746237.3746306
    - desc: Author Version
    - desc: Slides



- key: jasds2025-llm-trustworthiness
  bibtex: jasds2025-llm-trustworthiness.bib
  title: >-
    Integrating Natural Language Interfaces into Data Visualizations with Trustworthiness Scores
  date: '2025-11-03'
  authors:
    - Adrian Jobst
    - Daniel Atzberger
    - Willy Scheibel
    - Jürgen Döllner
    - Tobias Schreck
  published:
    - venue: 1st Workshop on GenAI, Agents, and the Future of VIS
      short: VISxGenAI '25
  articletype: Short Paper
  pending: false
  caption: >-
    We present a framework for integrating Natural Language Interfaces (NLIs) backed by
    Large Language Models (LLMs) into data visualizations to enhance accessibility for
    users with limited visualization literacy. Given a visualization specified in a
    structured JSON schema and its underlying data, the framework enables users to
    interact through natural language queries. The LLM interprets the query, decides
    whether to respond in text or modify the visualization by updating the specification,
    and generates answers by writing and executing code that analyzes the data. To enhance
    the reliability, the framework assigns a trustworthiness score to each response,
    derived from the LLM's performance on comparable tasks from the revised Visualization
    Literacy Assessment Test (Mini-VLAT). Our implementation supports both OpenAI and local
    models. We further discuss the pros and cons of these two alternatives concerning their
    use in NLIs. Our framework is intended for visualization designers working with charts
    of moderate complexity, such as those used in data journalism or embedded in websites,
    e.g., in our examples we included a Candlestick Chart for showing price movements of
    stocks.
  thumbnail: publications/2025-visxgenai-nlis-trustworthiness.png
  downloads:
    - desc: Workshop Version
      href: https://visxgenai.github.io/subs-2025/6449/6449-doc.pdf
    - desc: Author Version
    - desc: Poster



- key: wsd2026-smolvlm-parking-occupancy
  bibtex: wsd2026-smolvlm-parking-occupancy.bib
  title: >-
    Benchmarking SmolVLM for Parking Occupancy Detection
  date: '2026-01-29'
  authors:
    - Jobin Idiculla Wattasseril
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: 32nd International Conference on Multimedia Modeling
      short: MMM '26
      publisher: Springer
  articletype: Full Paper
  pending: true
  caption: >-
    Parking occupancy detection plays a critical role in optimizing urban spaces
    by enabling dynamic resource allocation and reducing traffic congestion.
    While prior approaches relying on lightweight architectures offer fast
    inference, their task-specific designs necessitate architectural modifications
    for new applications, limiting adaptability. Multimodal Large Language Models
    (MLLMs) have emerged as versatile alternatives, with efficient variants now
    deployable on edge devices. This paper presents the first benchmark of an
    existing efficient MLLM, SmolVLM, for the task of parking occupancy detection.
    We systematically evaluate the model's feasibility in a zero-shot setting,
    along with parameter-efficient fine-tuned variants across different model
    sizes and user/system prompt configurations, and further assess its ability
    for cross-dataset generalization. Experiments on two benchmark datasets,
    PKLot and CNRPark+EXT, demonstrate that our approach exhibits strong in-domain
    and cross-dataset performance, and either compete with or surpass prior
    task-specific architectures, despite training on significantly lesser data
    (91% reduction for PKLot; 40% reduction for CNRPark+EXT).
  thumbnail: publications/2026-mmm-smolvlm-parking-occupancy.png
  downloads:
    - desc: Publisher Record
    - desc: Author Version
    - desc: Replication Package
      href: https://osf.io/kdhe5?view_only=72e17f288d2f471eb4deb464d177a50c



- key: wsrd2026-multitemporal-dataset-survey
  bibtex: wsrd2026-multitemporal-dataset-survey.bib
  title: >-
    A Survey of Publicly Available Multi-Temporal Point Cloud Datasets
  date: '2026-01-01'
  authors:
    - Ole Wegen
    - Willy Scheibel
    - Rico Richter
    - Jürgen Döllner
  published:
    - venue: ISPRS Journal of Photogrammetry and Remote Sensing
      short: ISPRS P&RS
      publisher: Elsevier
  articletype: Journal Article
  pending: false
  doi: 10.1016/j.isprsjprs.2025.11.003
  highlight: true
  caption: >-
    Multi-temporal point clouds, which capture the same acquisition area at different points in time,
    enable change analysis and forecasting across various disciplines. Publicly available datasets
    play an important role in the development and evaluation of such approaches by enhancing
    comparability and reducing the effort required for data acquisition and preparation. However,
    identifying suitable datasets, assessing their characteristics, and comparing them with similar
    ones remains challenging and tedious due to the lack of a centralized distribution and
    documentation platform. In this paper, we provide a comprehensive overview of publicly available
    multi-temporal point cloud datasets. We evaluate each dataset across 30 different characteristics,
    grouped into six categories, and highlight current gaps and future challenges. Our analysis shows
    that, although many datasets are accompanied by extensive documentation, unclear usage terms and
    unreliable data hosting can limit their accessibility and adoption. In addition to clear
    correlations between application domains, acquisition methods, and captured scene types, there is
    also some overlap in point cloud requirements across domains. However, inconsistencies in file
    formats, data representations, and labeling practices hinder cross-domain and cross-application
    reuse. In the context of machine learning, we observe a positive trend toward more labeled
    datasets. Nevertheless, gaps remain due to limited coverage of natural environments and poor
    geographic diversity. Although there are already many positive examples of accessible datasets,
    future dataset publications would benefit from standardized review processes and a stronger focus
    on accessibility and usability across application areas.
  thumbnail: publications/2026-isprs-multitemporal-dataset-survey.jpg
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.1016/j.isprsjprs.2025.11.003
    - desc: Author Version
    - desc: Companion Website
      href: https://hpicgs.github.io/multi-temporal-point-cloud-datasets-survey/


- key: dwsucr2025-urban-space-analysis
  bibtex: dwsucr2025-urban-space-analysis.bib
  title: >-
    Design of an Intelligent System for Analyzing Urban Public Spaces Based on Feature Vector Abstraction
  date: '2025-11-24'
  authors:
    - Jürgen Döllner
    - Jobin Idiculla Wattasseril
    - Willy Scheibel
    - Daniel Ullmann
    - Jose Carvajal
    - Martin Reiter
  published:
    - venue: 15th International Conference on Electronics, Communications and Networks
      short: CECNet '25
      publisher: IOS Press
  articletype: Full Paper
  pending: false
  doi: 10.3233/FAIA251517
  highlight: false
  caption: >-
    Public spaces in cities are increasingly confronted with competing, intertwined uses, for example due to
    increasing vehicle traffic combined with a lack of parking spaces or constantly changing obstacles for
    pedestrian and vehicle traffic—which poses complex challenges for politics, administration, economy, and
    society as a whole. Our project aims strategically to address these challenges, particularly in the
    context of smart city initiatives, by developing an AI-based analysis and monitoring system for urban
    public spaces that classifies spatial events based on high-dimensional feature spaces. For given cameras,
    we extract content-related features from the image sequences using image analysis and store the results
    in the form of feature vector sequences. The interpretation and prediction of a camera's observations is
    based on a qualified cluster structure in the corresponding high-dimensional feature space, which is
    created by dimension reduction. During operation, only feature vectors are sent to a server, ensuring a
    high level of data protection and reducing data bandwidth requirements. If the system detects relevant
    events, activities such as requesting the corresponding video sequence can be triggered. The approach
    enables the long-term analysis of phenomena such as parking space occupancy, people occupancy, traffic
    density, and the detection of unusual events. It forms the basis for a generic tool for urban planners as
    well as authorities to analyze and visualize the dynamics of public spaces in cities.
  thumbnail: publications/2025-cecnet-urban-space-analysis.png
  downloads:
    - desc: Publisher Record
      href: https://doi.org/10.3233/FAIA251517
    - desc: Author Version


- key: 2026-llm-vis-evaluation
  bibtex: jasds2026-llm-vis-evaluation.bib
  date: '2026-03-09'
  title: "Evaluating Large Language Model Integration into Natural Language Interfaces for Visualizations with Available Input Data"
  authors:
    - Adrian Jobst
    - Daniel Atzberger
    - Willy Scheibel
    - Mariia Tytarenko
    - Jürgen Döllner
    - Tobias Schreck
  published:
    - venue: 21st International Conference on Computer Graphics, Interaction and Visualization Theory and Applications
      short: GRIVAPP '26
      publisher: SciTePress
  articletype: Short Paper
  pending: true
  highlight: false
  caption: >-
    We evaluate an approach for integrating Large Language Models (LLMs) into NLIs in cases where the underlying data is available. Thereby,
    the LLM is provided with an instruction prompt that defines the context and a visualization specification that formalizes the visual
    mapping.We compare this integration with approaches that rely on Multimodal LLMs (MLLMs) using the Visualization Literacy Assessment
    Test (VLAT) as test benchmark and further assess its performance with customized questionnaires that include questions with and without
    visual references and textual, as well as visual outputs. Our results demonstrate the effectiveness of our alternative approach and the
    superior performance compared to pure MLLM-based approaches. We further provide an implementation that serves as a blueprint for
    practitioners that aim to integrate LLMs into NLIs. The implementation is publicly available under
    https://github.com/hpicgs/lumos-visualization-llm-nli.
  thumbnail: publications/2026-grivapp-llm-vis-evaluation.png
  downloads:
    - desc: Publisher Record
    - desc: Author Version
    - desc: Presentation



- key: wsd2026-vigator-summarization
  bibtex: wsd2026-vigator-summarization.bib
  title: >-
    ViGATOR: Explainable Graph-based Video RAG and Summarization
  date: '2026-05-21'
  authors:
    - Jobin Idiculla Wattasseril
    - Willy Scheibel
    - Jürgen Döllner
  published:
    - venue: Proceedings of the 2026 Computer Vision Conference
      short: CVC '26
      publisher: Springer
  articletype: Full Paper
  pending: true
  caption: >-
    The proliferation of video data necessitates intelligent systems capable of complex, content-based retrieval
    and summarization. Videos are strongly structured in time, however, their semantic structure is largely implicit
    and only indirectly accessible. This complexity, along with temporal dynamics, makes indexing and summarization
    particularly challenging. This paper introduces ViGATOR, a framework that leverages pre-trained foundation models
    to construct a temporal knowledge graph from video content, capturing objects, their attributes, and interactions
    over time. This structured representation enables natural language querying, where user questions are translated
    into graph queries. A key advantage of our approach is its inherent explainability, as the system provides not
    only answers but also natural language rationales detailing the inference path. A case study on a real-world
    surveillance video demonstrates the system's effectiveness in handling complex temporal and relational queries.
  thumbnail: publications/2026-cvc-vigator-summarization.png
  downloads:
    - desc: Publisher Record
    - desc: Author Version
